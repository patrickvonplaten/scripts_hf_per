{"learning_rate": 0.0, "loss": 6.812309265136719, "step": 1}

Lines by line memory consumption:
/home/patrick/python_bin/transformers/trainer.py:218: mem 0.000B:         train_dataloader = self.get_train_dataloader()
/home/patrick/python_bin/transformers/trainer.py:157: mem 0.000B:         if self.train_dataset is None:
/home/patrick/python_bin/transformers/trainer.py:160: mem 0.000B:             RandomSampler(self.train_dataset) if self.args.local_rank == -1 else DistributedSampler(self.train_dataset)
/home/patrick/python_bin/transformers/trainer.py:162: mem 0.000B:         return DataLoader(
/home/patrick/python_bin/transformers/trainer.py:163: mem 0.000B:             self.train_dataset,
/home/patrick/python_bin/transformers/trainer.py:164: mem 0.000B:             batch_size=self.args.train_batch_size,
/home/patrick/python_bin/transformers/training_args.py:88: mem 0.000B:         return self.per_gpu_train_batch_size * max(1, self.n_gpu)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:122: mem 0.000B:         return self._setup_devices[1]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:524: mem 0.000B:             cached = self.fget(obj)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:97: mem 0.000B:         logger.info("PyTorch: setting up devices")
/home/patrick/python_bin/transformers/training_args.py:98: mem 0.000B:         if self.no_cuda:
/home/patrick/python_bin/transformers/training_args.py:101: mem 0.000B:         elif self.local_rank == -1:
/home/patrick/python_bin/transformers/training_args.py:104: mem 36.000KB:             device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
/home/patrick/python_bin/transformers/training_args.py:105: mem 0.000B:             n_gpu = torch.cuda.device_count()
/home/patrick/python_bin/transformers/training_args.py:112: mem 0.000B:         return device, n_gpu
/home/patrick/python_bin/transformers/file_utils.py:525: mem 0.000B:             setattr(obj, attr, cached)
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:165: mem 0.000B:             sampler=train_sampler,
/home/patrick/python_bin/transformers/trainer.py:166: mem 0.000B:             collate_fn=self.data_collator.collate_batch,
/home/patrick/python_bin/transformers/trainer.py:220: mem 0.000B:         if self.args.max_steps > 0:
/home/patrick/python_bin/transformers/trainer.py:226: mem 0.000B:             t_total = int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)
/home/patrick/python_bin/transformers/trainer.py:227: mem 0.000B:             num_train_epochs = self.args.num_train_epochs
/home/patrick/python_bin/transformers/trainer.py:229: mem 0.000B:         optimizer, scheduler = self.get_optimizers(num_training_steps=t_total)
/home/patrick/python_bin/transformers/trainer.py:192: mem 0.000B:         no_decay = ["bias", "LayerNorm.weight"]
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 76.000KB:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:195: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:196: mem 0.000B:                 "weight_decay": self.args.weight_decay,
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:199: mem 0.000B:                 "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
/home/patrick/python_bin/transformers/trainer.py:200: mem 0.000B:                 "weight_decay": 0.0,
/home/patrick/python_bin/transformers/trainer.py:203: mem 0.000B:         optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)
/home/patrick/python_bin/transformers/optimization.py:108: mem 0.000B:         if lr < 0.0:
/home/patrick/python_bin/transformers/optimization.py:110: mem 0.000B:         if not 0.0 <= betas[0] < 1.0:
/home/patrick/python_bin/transformers/optimization.py:112: mem 0.000B:         if not 0.0 <= betas[1] < 1.0:
/home/patrick/python_bin/transformers/optimization.py:114: mem 0.000B:         if not 0.0 <= eps:
/home/patrick/python_bin/transformers/optimization.py:116: mem 0.000B:         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)
/home/patrick/python_bin/transformers/optimization.py:117: mem 0.000B:         super().__init__(params, defaults)
/home/patrick/python_bin/transformers/trainer.py:204: mem 0.000B:         scheduler = get_linear_schedule_with_warmup(
/home/patrick/python_bin/transformers/trainer.py:205: mem 0.000B:             optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps
/home/patrick/python_bin/transformers/optimization.py:52: mem 0.000B:     def lr_lambda(current_step):
/home/patrick/python_bin/transformers/optimization.py:59: mem 0.000B:     return LambdaLR(optimizer, lr_lambda, last_epoch)
/home/patrick/python_bin/transformers/optimization.py:53: mem 0.000B:         if current_step < num_warmup_steps:
/home/patrick/python_bin/transformers/optimization.py:55: mem 0.000B:         return max(
/home/patrick/python_bin/transformers/optimization.py:56: mem 0.000B:             0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
/home/patrick/python_bin/transformers/optimization.py:53: mem 0.000B:         if current_step < num_warmup_steps:
/home/patrick/python_bin/transformers/optimization.py:55: mem 0.000B:         return max(
/home/patrick/python_bin/transformers/optimization.py:56: mem 0.000B:             0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
/home/patrick/python_bin/transformers/trainer.py:207: mem 0.000B:         return optimizer, scheduler
/home/patrick/python_bin/transformers/trainer.py:233: mem 0.000B:             model_path is not None
/home/patrick/python_bin/transformers/trainer.py:241: mem 0.000B:         model = self.model
/home/patrick/python_bin/transformers/trainer.py:242: mem 0.000B:         model.to(self.args.device)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:117: mem 0.000B:         return self._setup_devices[0]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:243: mem 0.000B:         if self.args.fp16:
/home/patrick/python_bin/transformers/trainer.py:249: mem 0.000B:         if self.args.n_gpu > 1:
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:122: mem 0.000B:         return self._setup_devices[1]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:253: mem 0.000B:         if self.args.local_rank != -1:
/home/patrick/python_bin/transformers/trainer.py:261: mem 0.000B:         if self.tb_writer is not None:
/home/patrick/python_bin/transformers/trainer.py:262: mem 0.000B:             self.tb_writer.add_text("args", self.args.to_json_string())
/home/patrick/python_bin/transformers/training_args.py:128: mem 0.000B:         return json.dumps(dataclasses.asdict(self), indent=2)
/home/patrick/python_bin/transformers/trainer.py:265: mem 0.000B:         logger.info("***** Running training *****")
/home/patrick/python_bin/transformers/trainer.py:266: mem 0.000B:         logger.info("  Num examples = %d", len(train_dataloader.dataset))
/home/patrick/python_bin/transformers/trainer.py:267: mem 0.000B:         logger.info("  Num Epochs = %d", num_train_epochs)
/home/patrick/python_bin/transformers/trainer.py:268: mem 0.000B:         logger.info("  Instantaneous batch size per GPU = %d", self.args.per_gpu_train_batch_size)
/home/patrick/python_bin/transformers/trainer.py:269: mem 0.000B:         logger.info(
/home/patrick/python_bin/transformers/trainer.py:270: mem 0.000B:             "  Total train batch size (w. parallel, distributed & accumulation) = %d",
/home/patrick/python_bin/transformers/trainer.py:273: mem 0.000B:             * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1),
/home/patrick/python_bin/transformers/training_args.py:88: mem 0.000B:         return self.per_gpu_train_batch_size * max(1, self.n_gpu)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:122: mem 0.000B:         return self._setup_devices[1]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:275: mem 0.000B:         logger.info("  Gradient Accumulation steps = %d", self.args.gradient_accumulation_steps)
/home/patrick/python_bin/transformers/trainer.py:276: mem 0.000B:         logger.info("  Total optimization steps = %d", t_total)
/home/patrick/python_bin/transformers/trainer.py:278: mem 0.000B:         global_step = 0
/home/patrick/python_bin/transformers/trainer.py:279: mem 0.000B:         epochs_trained = 0
/home/patrick/python_bin/transformers/trainer.py:280: mem 0.000B:         steps_trained_in_current_epoch = 0
/home/patrick/python_bin/transformers/trainer.py:282: mem 0.000B:         if model_path is not None:
/home/patrick/python_bin/transformers/trainer.py:299: mem 0.000B:         tr_loss = 0.0
/home/patrick/python_bin/transformers/trainer.py:300: mem 0.000B:         logging_loss = 0.0
/home/patrick/python_bin/transformers/trainer.py:301: mem 0.000B:         model.zero_grad()
/home/patrick/python_bin/transformers/trainer.py:302: mem 0.000B:         train_iterator = trange(
/home/patrick/python_bin/transformers/trainer.py:303: mem 0.000B:             epochs_trained, int(num_train_epochs), desc="Epoch", disable=self.args.local_rank not in [-1, 0],
/home/patrick/python_bin/transformers/trainer.py:305: mem 0.000B:         for epoch in train_iterator:
/home/patrick/python_bin/transformers/trainer.py:306: mem 716.000KB:             epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=self.args.local_rank not in [-1, 0])
/home/patrick/python_bin/transformers/trainer.py:307: mem 86.520MB:             for step, inputs in enumerate(epoch_iterator):
/home/patrick/python_bin/transformers/trainer.py:310: mem 0.000B:                 if steps_trained_in_current_epoch > 0:
/home/patrick/python_bin/transformers/trainer.py:314: mem 0.000B:                 tr_loss += self._training_step(model, inputs, optimizer)
/home/patrick/python_bin/transformers/trainer.py:384: mem 0.000B:         model.train()
/home/patrick/python_bin/transformers/trainer.py:385: mem 0.000B:         for k, v in inputs.items():
/home/patrick/python_bin/transformers/trainer.py:386: mem 0.000B:             inputs[k] = v.to(self.args.device)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:117: mem 0.000B:         return self._setup_devices[0]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:385: mem 0.000B:         for k, v in inputs.items():
/home/patrick/python_bin/transformers/trainer.py:386: mem 0.000B:             inputs[k] = v.to(self.args.device)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:117: mem 0.000B:         return self._setup_devices[0]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:385: mem 0.000B:         for k, v in inputs.items():
/home/patrick/python_bin/transformers/trainer.py:386: mem 0.000B:             inputs[k] = v.to(self.args.device)
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:117: mem 0.000B:         return self._setup_devices[0]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:385: mem 0.000B:         for k, v in inputs.items():
/home/patrick/python_bin/transformers/trainer.py:388: mem 264.000KB:         outputs = model(**inputs)
/home/patrick/python_bin/transformers/modeling_reformer.py:1370: mem 0.000B:         reformer_outputs = self.reformer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1371: mem 0.000B:             input_ids,
/home/patrick/python_bin/transformers/modeling_reformer.py:1372: mem 0.000B:             position_ids=position_ids,
/home/patrick/python_bin/transformers/modeling_reformer.py:1373: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1374: mem 0.000B:             head_masks=head_masks,
/home/patrick/python_bin/transformers/modeling_reformer.py:1375: mem 0.000B:             inputs_embeds=inputs_embeds,
/home/patrick/python_bin/transformers/modeling_reformer.py:1376: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1377: mem 0.000B:             do_output_hidden_states=do_output_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1378: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:1235: mem 0.000B:         if input_ids is not None and inputs_embeds is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:1237: mem 0.000B:         elif input_ids is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:1238: mem 0.000B:             input_shape = input_ids.size()  # noqa: F841
/home/patrick/python_bin/transformers/modeling_reformer.py:1244: mem 0.000B:         assert len(input_shape) == 2, "`input_ids` have be of shape `[batch_size, sequence_length]`, but got shape: {}".format(input_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:1246: mem 0.000B:         device = input_ids.device if input_ids is not None else inputs_embeds.device  # noqa: F841
/home/patrick/python_bin/transformers/modeling_reformer.py:1247: mem 0.000B:         real_sequence_length = input_shape[-1]
/home/patrick/python_bin/transformers/modeling_reformer.py:1250: mem 0.000B:         least_common_mult_chunk_length = _get_least_common_mult_chunk_len(self.config)
/home/patrick/python_bin/transformers/modeling_reformer.py:70: mem 0.000B:     attn_types = config.attn_layers
/home/patrick/python_bin/transformers/modeling_reformer.py:71: mem 0.000B:     if len(set(attn_types)) == 1 and attn_types[0] == "lsh":
/home/patrick/python_bin/transformers/modeling_reformer.py:73: mem 0.000B:     elif len(set(attn_types)) == 1 and attn_types[0] == "local":
/home/patrick/python_bin/transformers/modeling_reformer.py:75: mem 0.000B:     elif len(set(attn_types)) == 2 and set(attn_types) == set(["lsh", "local"]):
/home/patrick/python_bin/transformers/modeling_reformer.py:76: mem 0.000B:         return np.lcm(config.lsh_attn_chunk_length, config.local_attn_chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:1251: mem 0.000B:         if input_shape[-1] % least_common_mult_chunk_length != 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:1295: mem 264.000KB:         head_masks = self.get_head_mask(head_masks, self.config.num_hidden_layers)
/home/patrick/python_bin/transformers/modeling_utils.py:190: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_utils.py:193: mem 0.000B:             head_mask = [None] * num_hidden_layers
/home/patrick/python_bin/transformers/modeling_utils.py:195: mem 0.000B:         return head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:1297: mem 0.000B:         embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if input_ids is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:249: mem 0.000B:             input_shape = input_ids.size()
/home/patrick/python_bin/transformers/modeling_reformer.py:253: mem 0.000B:         seq_length = input_shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         device = input_ids.device if input_ids is not None else inputs_embeds.device
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         if position_ids is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:             position_ids = torch.arange(seq_length, dtype=torch.long, device=device)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:             position_ids = position_ids.unsqueeze(0).expand(input_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         if inputs_embeds is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 128.383MB:             inputs_embeds = self.word_embeddings(input_ids)
/home/patrick/python_bin/transformers/modeling_reformer.py:263: mem 0.000B:             position_ids.shape[-1] <= self.max_position_embeddings
/home/patrick/python_bin/transformers/modeling_reformer.py:267: mem 257.387MB:         embeddings = nn.functional.dropout(inputs_embeds, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:269: mem 0.000B:         position_embeddings = self.position_embeddings(position_ids)
/home/patrick/python_bin/transformers/modeling_reformer.py:167: mem 0.000B:         batch_size = position_ids.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:         sequence_length = position_ids.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:             weight.expand((batch_size,) + self.axial_pos_shape + weight.shape[-1:]) for weight in self.weights
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:             weight.expand((batch_size,) + self.axial_pos_shape + weight.shape[-1:]) for weight in self.weights
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:             weight.expand((batch_size,) + self.axial_pos_shape + weight.shape[-1:]) for weight in self.weights
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:             weight.expand((batch_size,) + self.axial_pos_shape + weight.shape[-1:]) for weight in self.weights
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:         if self.training is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:176: mem 0.000B:                 reduce(mul, self.axial_pos_shape) == sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:180: mem 0.000B:             if self.dropout > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:181: mem 128.238MB:                 weights = torch.cat(broadcasted_weights, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:                 perm_weigthts = weights.permute(0, 3, 2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:185: mem 128.039MB:                 drop_perm_weights = nn.functional.dropout2d(perm_weigthts, self.dropout, training=self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:186: mem 0.000B:                 drop_weights = drop_perm_weights.permute(0, 3, 2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:                 position_encodings = torch.reshape(drop_weights, (batch_size, sequence_length, -1))
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -127.746MB:         return position_encodings
/home/patrick/python_bin/transformers/modeling_reformer.py:271: mem 60.000KB:         embeddings = embeddings + position_embeddings
/home/patrick/python_bin/transformers/modeling_reformer.py:272: mem 128.062MB:         embeddings = nn.functional.dropout(embeddings, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem -256.008MB:         return embeddings
/home/patrick/python_bin/transformers/modeling_reformer.py:1299: mem 0.000B:         encoder_outputs = self.encoder(
/home/patrick/python_bin/transformers/modeling_reformer.py:1300: mem 0.000B:             hidden_states=embedding_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1301: mem 0.000B:             head_masks=head_masks,
/home/patrick/python_bin/transformers/modeling_reformer.py:1302: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1303: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1304: mem 0.000B:             do_output_hidden_states=do_output_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1305: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:1094: mem 0.000B:         all_hidden_states = []
/home/patrick/python_bin/transformers/modeling_reformer.py:1095: mem 0.000B:         all_attentions = []
/home/patrick/python_bin/transformers/modeling_reformer.py:1098: mem 256.004MB:         hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:1099: mem 0.000B:         hidden_states = _ReversibleFunction.apply(
/home/patrick/python_bin/transformers/modeling_reformer.py:1100: mem 0.000B:             hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1101: mem 0.000B:             self.layers,
/home/patrick/python_bin/transformers/modeling_reformer.py:1102: mem 0.000B:             attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1103: mem 0.000B:             head_masks,
/home/patrick/python_bin/transformers/modeling_reformer.py:1104: mem 0.000B:             num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1105: mem 0.000B:             all_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1106: mem 0.000B:             all_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:1107: mem 0.000B:             do_output_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1108: mem 0.000B:             do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:1001: mem 0.000B:         all_buckets = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:1002: mem 0.000B:         hidden_states, attn_output = torch.chunk(hidden_states, 2, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 128.258MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:666: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:667: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:669: mem 65.203MB:         mixed_query_vectors = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:670: mem 64.496MB:         mixed_key_vectors = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:671: mem 63.938MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:673: mem 0.000B:         query_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:674: mem 0.000B:             mixed_query_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:676: mem 0.000B:         key_vectors = self._transpose_for_scores(mixed_key_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:677: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:678: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:681: mem 0.000B:         assert query_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:682: mem 0.000B:         assert key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:683: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:685: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:688: mem 0.000B:         key_vectors = key_vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:689: mem 64.762MB:             torch.tensor(self.attention_head_size, device=key_vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:693: mem 0.000B:         query_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:694: mem 0.000B:             query_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:696: mem 0.000B:         key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:697: mem 0.000B:             key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:699: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:700: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:704: mem 0.000B:         indices = torch.arange(sequence_length, device=query_vectors.device).repeat(
/home/patrick/python_bin/transformers/modeling_reformer.py:705: mem 0.000B:             batch_size, self.num_attention_heads, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:707: mem 0.000B:         query_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:708: mem 0.000B:         key_value_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:711: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.035MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem -3.914MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:712: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.250MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:713: mem 0.000B:         key_value_indices = self._look_adjacent(key_value_indices, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:716: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:717: mem 0.000B:             attention_mask = attention_mask.to(torch.uint8)[:, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:718: mem 0.000B:             attention_mask = self._split_dim_by(attention_mask, -1, self.chunk_length, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:719: mem 0.000B:             attention_mask = self._look_adjacent(attention_mask, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:722: mem 128.203MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:724: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:726: mem 0.000B:         if self.is_decoder is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:728: mem 0.000B:             mask = torch.ge(query_indices.unsqueeze(-1), key_value_indices.unsqueeze(-2)).to(query_indices.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:731: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:732: mem 0.000B:             attn_mask = attention_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:734: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:735: mem 32.266MB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:739: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:740: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:741: mem 320.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:744: mem 936.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:745: mem 128.344MB:         attention_probs = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:748: mem 60.000KB:         attention_probs = nn.functional.dropout(attention_probs, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:751: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:755: mem 63.934MB:         out_vectors = torch.matmul(attention_probs, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:758: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:759: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:761: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:763: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 132.000KB:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:765: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:766: mem -128.004MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:768: mem -608.027MB:         return LocalSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem 132.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:836: mem 0.000B:             buckets = None
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.004MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 127.871MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 128.133MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 256.781MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem 196.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem -132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 128.129MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -384.074MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.000MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem 0.000B:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem 0.000B:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 127.871MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:370: mem 0.000B:         num_hashes = num_hashes if num_hashes is not None else self.num_hashes
/home/patrick/python_bin/transformers/modeling_reformer.py:372: mem 63.938MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:373: mem 64.195MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:375: mem 0.000B:         query_key_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:376: mem 0.000B:             mixed_query_key_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:378: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:379: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         if self.num_buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:390: mem 0.000B:         if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:392: mem 0.000B:             buckets = self._hash_vectors(query_key_vectors, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:446: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:451: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:459: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:467: mem 0.000B:         rotations_shape = (vectors.shape[-1], num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:470: mem 0.000B:         if self.hash_seed is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:477: mem 0.000B:             random_rotations = torch.randn(rotations_shape, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:484: mem 96.133MB:         rotated_vectors = torch.einsum("bmtd,dhr->bmhtr", vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:486: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:491: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem -31.926MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:498: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem 88.000KB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:500: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:506: mem 0.000B:         offsets = torch.arange(num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:507: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:510: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:511: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:513: mem -64.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         assert int(buckets.shape[-1]) == num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:516: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:519: mem 0.000B:         ticker = torch.arange(num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:520: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:522: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:525: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:526: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:528: mem 0.000B:         sorted_ticker = sorted_ticker % sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:529: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 64.215MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 124.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 63.934MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 36.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:401: mem 0.000B:         query_key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:402: mem 0.000B:             query_key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:404: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:405: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:625: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:632: mem 596.000KB:         variance = torch.mean(x ** 2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:633: mem 63.934MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:634: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:626: mem 0.000B:         vectors = vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:627: mem 0.000B:             torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:629: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:415: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(
/home/patrick/python_bin/transformers/modeling_reformer.py:416: mem 0.000B:             query_vectors=query_key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:417: mem 0.000B:             key_vectors=key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:418: mem 0.000B:             value_vectors=value_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:419: mem 0.000B:             ticker=ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:420: mem 0.000B:             undo_ticker=undo_ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:421: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:422: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:544: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.062MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:545: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.059MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:548: mem 127.871MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:551: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:552: mem 0.000B:         key_value_info = self._look_adjacent(ticker, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:554: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:556: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:558: mem 0.000B:             mask = torch.ge(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:562: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:563: mem 0.000B:             attn_mask = attention_mask.to(torch.uint8)[:, None, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:565: mem 0.000B:             attn_mask = attn_mask.expand(ticker.shape[:-1] + (-1,))
/home/patrick/python_bin/transformers/modeling_reformer.py:566: mem 0.000B:             attn_mask = torch.gather(attn_mask, -1, key_value_info)
/home/patrick/python_bin/transformers/modeling_reformer.py:568: mem 0.000B:             attn_mask = attn_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:570: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:571: mem 32.277MB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:576: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:577: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:578: mem -64.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:593: mem -31.824MB:         mask = torch.ne(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:594: mem 0.000B:         query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:595: mem -72.000KB:             mask, query_key_dots, torch.tensor(-1e5, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:598: mem 128.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:600: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:605: mem 60.000KB:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:608: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:612: mem 63.934MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:615: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:616: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:618: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:619: mem 40.000KB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:620: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:622: mem -384.012MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:425: mem 0.000B:         if num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:438: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem -36.000KB:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:440: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:441: mem -128.000MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:443: mem -320.020MB:         return LSHSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs, buckets=buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:834: mem 0.000B:             buckets = self_attention_outputs.buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.008MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 128.129MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 127.875MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 256.008MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem 140.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem -132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 128.129MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem -132.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -383.816MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.004MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem -128.004MB:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem -128.004MB:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 127.871MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:666: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:667: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:669: mem 63.938MB:         mixed_query_vectors = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:670: mem 63.938MB:         mixed_key_vectors = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:671: mem 63.938MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:673: mem 0.000B:         query_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:674: mem 0.000B:             mixed_query_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:676: mem 0.000B:         key_vectors = self._transpose_for_scores(mixed_key_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:677: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:678: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:681: mem 0.000B:         assert query_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:682: mem 0.000B:         assert key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:683: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:685: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:688: mem 0.000B:         key_vectors = key_vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:689: mem 63.938MB:             torch.tensor(self.attention_head_size, device=key_vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:693: mem 0.000B:         query_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:694: mem 0.000B:             query_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:696: mem 0.000B:         key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:697: mem 0.000B:             key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:699: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:700: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:704: mem 0.000B:         indices = torch.arange(sequence_length, device=query_vectors.device).repeat(
/home/patrick/python_bin/transformers/modeling_reformer.py:705: mem 0.000B:             batch_size, self.num_attention_heads, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:707: mem 0.000B:         query_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:708: mem 0.000B:         key_value_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:711: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.195MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 208.000KB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:712: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 63.992MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:713: mem 0.000B:         key_value_indices = self._look_adjacent(key_value_indices, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:716: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:717: mem 0.000B:             attention_mask = attention_mask.to(torch.uint8)[:, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:718: mem 0.000B:             attention_mask = self._split_dim_by(attention_mask, -1, self.chunk_length, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:719: mem 0.000B:             attention_mask = self._look_adjacent(attention_mask, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:722: mem 128.105MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:724: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:726: mem 0.000B:         if self.is_decoder is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:728: mem 0.000B:             mask = torch.ge(query_indices.unsqueeze(-1), key_value_indices.unsqueeze(-2)).to(query_indices.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:731: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:732: mem 0.000B:             attn_mask = attention_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:734: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:735: mem 31.973MB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:739: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:740: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:741: mem -68.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:744: mem 128.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:745: mem 127.871MB:         attention_probs = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:748: mem 60.000KB:         attention_probs = nn.functional.dropout(attention_probs, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:751: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:755: mem 63.934MB:         out_vectors = torch.matmul(attention_probs, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:758: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:759: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:761: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:763: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:765: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:766: mem -128.000MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:768: mem -608.027MB:         return LocalSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem 132.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:836: mem 0.000B:             buckets = None
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.004MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 127.871MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 128.133MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 256.008MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem 136.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem -132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem 132.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -384.070MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.004MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem -128.004MB:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem -128.004MB:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 128.129MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:370: mem 0.000B:         num_hashes = num_hashes if num_hashes is not None else self.num_hashes
/home/patrick/python_bin/transformers/modeling_reformer.py:372: mem 63.938MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:373: mem 63.938MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:375: mem 0.000B:         query_key_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:376: mem 0.000B:             mixed_query_key_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:378: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:379: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         if self.num_buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:390: mem 0.000B:         if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:392: mem 0.000B:             buckets = self._hash_vectors(query_key_vectors, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:446: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:451: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:459: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:467: mem 0.000B:         rotations_shape = (vectors.shape[-1], num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:470: mem 0.000B:         if self.hash_seed is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:477: mem 0.000B:             random_rotations = torch.randn(rotations_shape, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:484: mem 96.066MB:         rotated_vectors = torch.einsum("bmtd,dhr->bmhtr", vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:486: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:491: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem -32.035MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:498: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem 232.000KB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:500: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:506: mem 0.000B:         offsets = torch.arange(num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:507: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:510: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:511: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:513: mem -64.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         assert int(buckets.shape[-1]) == num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:516: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:519: mem 0.000B:         ticker = torch.arange(num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:520: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:522: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:525: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:526: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:528: mem 0.000B:         sorted_ticker = sorted_ticker % sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:529: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 63.934MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 40.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 63.934MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 40.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:401: mem 0.000B:         query_key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:402: mem 0.000B:             query_key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:404: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:405: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:625: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:632: mem -28.000KB:         variance = torch.mean(x ** 2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:633: mem 63.934MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:634: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:626: mem 0.000B:         vectors = vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:627: mem 0.000B:             torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:629: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:415: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(
/home/patrick/python_bin/transformers/modeling_reformer.py:416: mem 0.000B:             query_vectors=query_key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:417: mem 0.000B:             key_vectors=key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:418: mem 0.000B:             value_vectors=value_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:419: mem 0.000B:             ticker=ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:420: mem 0.000B:             undo_ticker=undo_ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:421: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:422: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:544: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.062MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:545: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.059MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:548: mem 128.129MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:551: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:552: mem 0.000B:         key_value_info = self._look_adjacent(ticker, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:554: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:556: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:558: mem 0.000B:             mask = torch.ge(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:562: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:563: mem 0.000B:             attn_mask = attention_mask.to(torch.uint8)[:, None, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:565: mem 0.000B:             attn_mask = attn_mask.expand(ticker.shape[:-1] + (-1,))
/home/patrick/python_bin/transformers/modeling_reformer.py:566: mem 0.000B:             attn_mask = torch.gather(attn_mask, -1, key_value_info)
/home/patrick/python_bin/transformers/modeling_reformer.py:568: mem 0.000B:             attn_mask = attn_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:570: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:571: mem 31.980MB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:576: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:577: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:578: mem -68.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:593: mem -32.004MB:         mask = torch.ne(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:594: mem 0.000B:         query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:595: mem 196.000KB:             mask, query_key_dots, torch.tensor(-1e5, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:598: mem -136.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:600: mem 128.129MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:605: mem -204.000KB:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:608: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:612: mem 64.191MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:615: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:616: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:618: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:619: mem 44.000KB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:620: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:622: mem -384.012MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:425: mem 0.000B:         if num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:438: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem -36.000KB:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:440: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:441: mem -128.004MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:443: mem -320.020MB:         return LSHSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs, buckets=buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:834: mem 0.000B:             buckets = self_attention_outputs.buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.004MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 127.871MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 127.875MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 256.266MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem -128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem 132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -384.070MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.004MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem -128.004MB:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem -128.004MB:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 127.871MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:666: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:667: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:669: mem 63.938MB:         mixed_query_vectors = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:670: mem 64.195MB:         mixed_key_vectors = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:671: mem 63.938MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:673: mem 0.000B:         query_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:674: mem 0.000B:             mixed_query_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:676: mem 0.000B:         key_vectors = self._transpose_for_scores(mixed_key_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:677: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:678: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:681: mem 0.000B:         assert query_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:682: mem 0.000B:         assert key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:683: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:685: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:688: mem 0.000B:         key_vectors = key_vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:689: mem 63.938MB:             torch.tensor(self.attention_head_size, device=key_vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:693: mem 0.000B:         query_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:694: mem 0.000B:             query_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:696: mem 0.000B:         key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:697: mem 0.000B:             key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:699: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:700: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:704: mem 0.000B:         indices = torch.arange(sequence_length, device=query_vectors.device).repeat(
/home/patrick/python_bin/transformers/modeling_reformer.py:705: mem 0.000B:             batch_size, self.num_attention_heads, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:707: mem 0.000B:         query_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:708: mem 0.000B:         key_value_indices = self._split_dim_by(indices, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:711: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.195MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem -56.000KB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:712: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:713: mem 0.000B:         key_value_indices = self._look_adjacent(key_value_indices, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:716: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:717: mem 0.000B:             attention_mask = attention_mask.to(torch.uint8)[:, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:718: mem 0.000B:             attention_mask = self._split_dim_by(attention_mask, -1, self.chunk_length, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:719: mem 0.000B:             attention_mask = self._look_adjacent(attention_mask, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:722: mem 128.105MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:724: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:726: mem 0.000B:         if self.is_decoder is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:728: mem 31.965MB:             mask = torch.ge(query_indices.unsqueeze(-1), key_value_indices.unsqueeze(-2)).to(query_indices.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:731: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:732: mem 0.000B:             attn_mask = attention_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:734: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:735: mem -20.000KB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:739: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:740: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:741: mem 196.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:744: mem -136.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:745: mem 128.129MB:         attention_probs = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:748: mem -204.000KB:         attention_probs = nn.functional.dropout(attention_probs, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:751: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:755: mem 64.191MB:         out_vectors = torch.matmul(attention_probs, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:758: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:759: mem 36.000KB:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:761: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:763: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem -24.000KB:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:765: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:766: mem -128.004MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:768: mem -608.027MB:         return LocalSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:836: mem 0.000B:             buckets = None
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.008MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 127.871MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 127.875MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 256.266MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem -128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem 132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 127.871MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem 128.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -384.074MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.004MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem -128.004MB:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem -128.004MB:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1004: mem 0.000B:             if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1008: mem 0.000B:             layer_outputs = layer(
/home/patrick/python_bin/transformers/modeling_reformer.py:1009: mem 0.000B:                 prev_attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:1010: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:1011: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1012: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:1013: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:1014: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:909: mem 0.000B:         with torch.no_grad():
/home/patrick/python_bin/transformers/modeling_reformer.py:910: mem 0.000B:             attn_outputs = self.attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:911: mem 0.000B:                 hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:912: mem 0.000B:                 head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:913: mem 0.000B:                 attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:914: mem 0.000B:                 num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:915: mem 0.000B:                 do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 127.871MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:821: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:822: mem 0.000B:             hidden_states=norm_hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:823: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:824: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:825: mem 0.000B:             num_hashes=num_hashes,
/home/patrick/python_bin/transformers/modeling_reformer.py:826: mem 0.000B:             do_output_attentions=do_output_attentions,
/home/patrick/python_bin/transformers/modeling_reformer.py:827: mem 0.000B:             buckets=buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:370: mem 0.000B:         num_hashes = num_hashes if num_hashes is not None else self.num_hashes
/home/patrick/python_bin/transformers/modeling_reformer.py:372: mem 63.938MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:373: mem 64.195MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:375: mem 0.000B:         query_key_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:376: mem 0.000B:             mixed_query_key_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:378: mem 0.000B:         value_vectors = self._transpose_for_scores(
/home/patrick/python_bin/transformers/modeling_reformer.py:379: mem 0.000B:             mixed_value_vectors, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (num_attn_heads, attn_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         if self.num_buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:390: mem 0.000B:         if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:392: mem 0.000B:             buckets = self._hash_vectors(query_key_vectors, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:446: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:451: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:459: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:461: mem 0.000B:                 assert num_bucket % 2 == 0, "The number of buckets should be even, but `num_bucket`: {}".format(
/home/patrick/python_bin/transformers/modeling_reformer.py:464: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:465: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:460: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:467: mem 0.000B:         rotations_shape = (vectors.shape[-1], num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:470: mem 0.000B:         if self.hash_seed is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:477: mem 0.000B:             random_rotations = torch.randn(rotations_shape, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:484: mem 96.070MB:         rotated_vectors = torch.einsum("bmtd,dhr->bmhtr", vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:486: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:491: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem -32.031MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:498: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:493: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum : cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:494: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:495: mem -32.000KB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:497: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:500: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:502: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:492: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:506: mem 0.000B:         offsets = torch.arange(num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:507: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:510: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:511: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:513: mem -64.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         assert int(buckets.shape[-1]) == num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:516: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:519: mem 0.000B:         ticker = torch.arange(num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:520: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:522: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:525: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:526: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:528: mem 0.000B:         sorted_ticker = sorted_ticker % sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:529: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 63.934MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 48.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker, num_hashes)
/home/patrick/python_bin/transformers/modeling_reformer.py:637: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:638: mem 63.934MB:         vectors = vectors.repeat(1, 1, num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:639: mem 36.000KB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:401: mem 0.000B:         query_key_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:402: mem 0.000B:             query_key_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:404: mem 0.000B:         value_vectors = self._split_dim_by(
/home/patrick/python_bin/transformers/modeling_reformer.py:405: mem 0.000B:             value_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (attn_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, num_attn_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:625: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:632: mem 228.000KB:         variance = torch.mean(x ** 2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:633: mem 63.934MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:634: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:626: mem 0.000B:         vectors = vectors / torch.sqrt(
/home/patrick/python_bin/transformers/modeling_reformer.py:627: mem 8.000KB:             torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:629: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:415: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(
/home/patrick/python_bin/transformers/modeling_reformer.py:416: mem 0.000B:             query_vectors=query_key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:417: mem 0.000B:             key_vectors=key_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:418: mem 0.000B:             value_vectors=value_vectors,
/home/patrick/python_bin/transformers/modeling_reformer.py:419: mem 0.000B:             ticker=ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:420: mem 0.000B:             undo_ticker=undo_ticker,
/home/patrick/python_bin/transformers/modeling_reformer.py:421: mem 0.000B:             attention_mask=attention_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:422: mem 0.000B:             head_mask=head_mask,
/home/patrick/python_bin/transformers/modeling_reformer.py:544: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.059MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:545: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.062MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:548: mem 127.871MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:551: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:552: mem 0.000B:         key_value_info = self._look_adjacent(ticker, self.num_chunks_before, self.num_chunks_after)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         if num_chunks_before == 0 and num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         for i in range(-num_chunks_before, num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:554: mem 0.000B:         mask = None
/home/patrick/python_bin/transformers/modeling_reformer.py:556: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:558: mem 32.223MB:             mask = torch.ge(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:562: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:563: mem 0.000B:             attn_mask = attention_mask.to(torch.uint8)[:, None, None, :]
/home/patrick/python_bin/transformers/modeling_reformer.py:565: mem 0.000B:             attn_mask = attn_mask.expand(ticker.shape[:-1] + (-1,))
/home/patrick/python_bin/transformers/modeling_reformer.py:566: mem 0.000B:             attn_mask = torch.gather(attn_mask, -1, key_value_info)
/home/patrick/python_bin/transformers/modeling_reformer.py:568: mem 0.000B:             attn_mask = attn_mask.unsqueeze(-2).expand(query_key_dots.shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:570: mem 0.000B:             if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:571: mem -16.000KB:                 mask = mask * attn_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:576: mem 0.000B:         if mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:577: mem 0.000B:             query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:578: mem -68.000KB:                 mask, query_key_dots, torch.tensor(-1e9, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:593: mem -20.000KB:         mask = torch.ne(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:594: mem 0.000B:         query_key_dots = torch.where(
/home/patrick/python_bin/transformers/modeling_reformer.py:595: mem -68.000KB:             mask, query_key_dots, torch.tensor(-1e5, dtype=query_key_dots.dtype, device=query_key_dots.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:598: mem 128.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:600: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:605: mem 64.000KB:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:608: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:612: mem 63.934MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:615: mem 0.000B:         logits = self._merge_by_middle_dim(logits, self.num_attention_heads).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:616: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors, self.num_attention_heads)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, num_attn_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:618: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:619: mem 44.000KB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:620: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:622: mem -416.016MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:425: mem 0.000B:         if num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:438: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors, self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem -32.000KB:         return torch.reshape(x, (x.size()[0], -1, num_attn_heads * attn_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:440: mem 0.000B:         if do_output_attentions is False:
/home/patrick/python_bin/transformers/modeling_reformer.py:441: mem -128.004MB:             attention_probs = ()
/home/patrick/python_bin/transformers/modeling_reformer.py:443: mem -320.020MB:         return LSHSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs, buckets=buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:830: mem 0.000B:         attention_output = self.output(self_attention_outputs.hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:779: mem 128.129MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:780: mem -136.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:781: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:833: mem 0.000B:         if hasattr(self_attention_outputs, "buckets"):
/home/patrick/python_bin/transformers/modeling_reformer.py:834: mem 0.000B:             buckets = self_attention_outputs.buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:838: mem 0.000B:         return AttentionOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -192.000MB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:917: mem 0.000B:             attn_output = attn_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 128.129MB:             attn_output = prev_attn_output + attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:923: mem 0.000B:             hidden_states = hidden_states + self.feed_forward(attn_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:882: mem 0.000B:         return apply_chunking_to_forward(
/home/patrick/python_bin/transformers/modeling_reformer.py:883: mem 0.000B:             self.chunk_size_feed_forward, self.seq_len_dim, self.forward_chunk, attention_output
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 127.875MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:888: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 255.750MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:855: mem 392.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:856: mem -132.000KB:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:857: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:889: mem 0.000B:         output = self.output(dense_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 128.129MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:868: mem -136.000KB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:869: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -383.816MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:925: mem 0.000B:         return ReformerOutput(
/home/patrick/python_bin/transformers/modeling_reformer.py:926: mem 0.000B:             attn_output=attn_output,
/home/patrick/python_bin/transformers/modeling_reformer.py:927: mem 0.000B:             hidden_states=hidden_states,
/home/patrick/python_bin/transformers/modeling_reformer.py:928: mem 0.000B:             attention_probs=attn_outputs.attention_probs,
/home/patrick/python_bin/transformers/modeling_reformer.py:929: mem -128.000MB:             buckets=attn_outputs.buckets,
/home/patrick/python_bin/transformers/modeling_reformer.py:1016: mem -128.004MB:             attn_output = layer_outputs.attn_output
/home/patrick/python_bin/transformers/modeling_reformer.py:1017: mem -128.004MB:             hidden_states = layer_outputs.hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1018: mem 0.000B:             all_buckets = all_buckets + (layer_outputs.buckets,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1020: mem 0.000B:             if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1003: mem 0.000B:         for layer, head_mask in zip(layers, head_masks):
/home/patrick/python_bin/transformers/modeling_reformer.py:1024: mem 0.000B:         if do_output_hidden_states is True:
/home/patrick/python_bin/transformers/modeling_reformer.py:1029: mem 0.000B:         ctx.attention_mask = attention_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:1030: mem 0.000B:         ctx.head_masks = head_masks
/home/patrick/python_bin/transformers/modeling_reformer.py:1031: mem 0.000B:         ctx.attn_output = attn_output.detach()
/home/patrick/python_bin/transformers/modeling_reformer.py:1032: mem 0.000B:         ctx.hidden_states = hidden_states.detach()
/home/patrick/python_bin/transformers/modeling_reformer.py:1033: mem 0.000B:         ctx.layers = layers
/home/patrick/python_bin/transformers/modeling_reformer.py:1034: mem 0.000B:         ctx.all_buckets = all_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:1037: mem 256.059MB:         hidden_states = torch.cat([attn_output, hidden_states], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:1038: mem -255.996MB:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1112: mem 255.746MB:         hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:1115: mem 256.266MB:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:1117: mem 0.000B:         outputs = (hidden_states,)
/home/patrick/python_bin/transformers/modeling_reformer.py:1118: mem 0.000B:         if do_output_hidden_states:
/home/patrick/python_bin/transformers/modeling_reformer.py:1120: mem 0.000B:         if do_output_attentions:
/home/patrick/python_bin/transformers/modeling_reformer.py:1122: mem 0.000B:         return outputs  # last-layer hidden state, (all hidden states), (all attentions)
/home/patrick/python_bin/transformers/modeling_reformer.py:1307: mem 0.000B:         sequence_output = encoder_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:1310: mem 0.000B:         if real_sequence_length < input_shape[-1]:
/home/patrick/python_bin/transformers/modeling_reformer.py:1314: mem 0.000B:         outputs = (sequence_output,) + encoder_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:1315: mem -127.996MB:         return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)
/home/patrick/python_bin/transformers/modeling_reformer.py:1381: mem 0.000B:         sequence_output = reformer_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:1382: mem 0.000B:         lm_logits = self.lm_head(sequence_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:1332: mem 0.000B:         return apply_chunking_to_forward(self.chunk_size_lm_head, self.seq_len_dim, self.forward_chunk, hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:103: mem 0.000B:     assert len(input_tensors) > 0, "{} has to be a tuple/list of tensors".format(input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:104: mem 0.000B:     tensor_shape = input_tensors[0].shape
/home/patrick/python_bin/transformers/modeling_reformer.py:105: mem 0.000B:     assert all(
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         input_tensor.shape == tensor_shape for input_tensor in input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:110: mem 0.000B:     num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:     assert num_args_in_forward_chunk_fn == len(
/home/patrick/python_bin/transformers/modeling_reformer.py:112: mem 0.000B:         input_tensors
/home/patrick/python_bin/transformers/modeling_reformer.py:117: mem 0.000B:     if chunk_size > 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:133: mem 0.000B:     return forward_fn(*input_tensors)
/home/patrick/python_bin/transformers/modeling_reformer.py:1336: mem 159.840MB:         hidden_states = self.decoder(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:1337: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:1383: mem 0.000B:         outputs = (lm_logits,) + reformer_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:1385: mem 0.000B:         if lm_labels is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:1387: mem 0.000B:             shift_logits = lm_logits[..., :-1, :].contiguous()
/home/patrick/python_bin/transformers/modeling_reformer.py:1388: mem 0.000B:             shift_labels = lm_labels[..., 1:].contiguous()
/home/patrick/python_bin/transformers/modeling_reformer.py:1390: mem 0.000B:             loss_fct = CrossEntropyLoss()
/home/patrick/python_bin/transformers/modeling_reformer.py:1391: mem 160.105MB:             loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
/home/patrick/python_bin/transformers/modeling_reformer.py:1392: mem 0.000B:             outputs = (loss,) + outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:1394: mem 0.000B:         return outputs  # (lm_loss), lm_logits, (hidden_states), (attentions)
/home/patrick/python_bin/transformers/trainer.py:389: mem 0.000B:         loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
/home/patrick/python_bin/transformers/trainer.py:391: mem 0.000B:         if self.args.n_gpu > 1:
/home/patrick/python_bin/transformers/file_utils.py:533: mem 0.000B:         if is_torch_available():
/home/patrick/python_bin/transformers/file_utils.py:104: mem 0.000B:     return _torch_available
/home/patrick/python_bin/transformers/file_utils.py:534: mem 0.000B:             return func(*args, **kwargs)
/home/patrick/python_bin/transformers/training_args.py:122: mem 0.000B:         return self._setup_devices[1]
/home/patrick/python_bin/transformers/file_utils.py:517: mem 0.000B:         if obj is None:
/home/patrick/python_bin/transformers/file_utils.py:519: mem 0.000B:         if self.fget is None:
/home/patrick/python_bin/transformers/file_utils.py:521: mem 0.000B:         attr = "__cached_" + self.fget.__name__
/home/patrick/python_bin/transformers/file_utils.py:522: mem 0.000B:         cached = getattr(obj, attr, None)
/home/patrick/python_bin/transformers/file_utils.py:523: mem 0.000B:         if cached is None:
/home/patrick/python_bin/transformers/file_utils.py:526: mem 0.000B:         return cached
/home/patrick/python_bin/transformers/trainer.py:393: mem 0.000B:         if self.args.gradient_accumulation_steps > 1:
/home/patrick/python_bin/transformers/trainer.py:396: mem 0.000B:         if self.args.fp16:
/home/patrick/python_bin/transformers/trainer.py:400: mem -1.128GB:             loss.backward()
/home/patrick/python_bin/transformers/trainer.py:402: mem -415.996MB:         return loss.item()
/home/patrick/python_bin/transformers/trainer.py:316: mem 0.000B:                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (
/home/patrick/python_bin/transformers/trainer.py:321: mem 0.000B:                     if self.args.fp16:
/home/patrick/python_bin/transformers/trainer.py:324: mem 0.000B:                         torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
/home/patrick/python_bin/transformers/trainer.py:326: mem 0.000B:                     optimizer.step()
/home/patrick/python_bin/transformers/optimization.py:126: mem 0.000B:         loss = None
/home/patrick/python_bin/transformers/optimization.py:127: mem 0.000B:         if closure is not None:
/home/patrick/python_bin/transformers/optimization.py:130: mem 0.000B:         for group in self.param_groups:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 436.000KB:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 120.000KB:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:130: mem 0.000B:         for group in self.param_groups:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:132: mem 0.000B:                 if p.grad is None:
/home/patrick/python_bin/transformers/optimization.py:134: mem 0.000B:                 grad = p.grad.data
/home/patrick/python_bin/transformers/optimization.py:135: mem 0.000B:                 if grad.is_sparse:
/home/patrick/python_bin/transformers/optimization.py:138: mem 0.000B:                 state = self.state[p]
/home/patrick/python_bin/transformers/optimization.py:141: mem 0.000B:                 if len(state) == 0:
/home/patrick/python_bin/transformers/optimization.py:142: mem 0.000B:                     state["step"] = 0
/home/patrick/python_bin/transformers/optimization.py:144: mem 0.000B:                     state["exp_avg"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:146: mem 0.000B:                     state["exp_avg_sq"] = torch.zeros_like(p.data)
/home/patrick/python_bin/transformers/optimization.py:148: mem 0.000B:                 exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
/home/patrick/python_bin/transformers/optimization.py:149: mem 0.000B:                 beta1, beta2 = group["betas"]
/home/patrick/python_bin/transformers/optimization.py:151: mem 0.000B:                 state["step"] += 1
/home/patrick/python_bin/transformers/optimization.py:155: mem 0.000B:                 exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/home/patrick/python_bin/transformers/optimization.py:156: mem 0.000B:                 exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)
/home/patrick/python_bin/transformers/optimization.py:157: mem 0.000B:                 denom = exp_avg_sq.sqrt().add_(group["eps"])
/home/patrick/python_bin/transformers/optimization.py:159: mem 0.000B:                 step_size = group["lr"]
/home/patrick/python_bin/transformers/optimization.py:160: mem 0.000B:                 if group["correct_bias"]:  # No bias correction for Bert
/home/patrick/python_bin/transformers/optimization.py:161: mem 0.000B:                     bias_correction1 = 1.0 - beta1 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:162: mem 0.000B:                     bias_correction2 = 1.0 - beta2 ** state["step"]
/home/patrick/python_bin/transformers/optimization.py:163: mem 0.000B:                     step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
/home/patrick/python_bin/transformers/optimization.py:165: mem 0.000B:                 p.data.addcdiv_(-step_size, exp_avg, denom)
/home/patrick/python_bin/transformers/optimization.py:175: mem 0.000B:                 if group["weight_decay"] > 0.0:
/home/patrick/python_bin/transformers/optimization.py:131: mem 0.000B:             for p in group["params"]:
/home/patrick/python_bin/transformers/optimization.py:130: mem 0.000B:         for group in self.param_groups:
/home/patrick/python_bin/transformers/optimization.py:178: mem 0.000B:         return loss
/home/patrick/python_bin/transformers/trainer.py:327: mem 0.000B:                     scheduler.step()
/home/patrick/python_bin/transformers/optimization.py:53: mem 0.000B:         if current_step < num_warmup_steps:
/home/patrick/python_bin/transformers/optimization.py:55: mem 0.000B:         return max(
/home/patrick/python_bin/transformers/optimization.py:56: mem 0.000B:             0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
/home/patrick/python_bin/transformers/optimization.py:53: mem 0.000B:         if current_step < num_warmup_steps:
/home/patrick/python_bin/transformers/optimization.py:55: mem 0.000B:         return max(
/home/patrick/python_bin/transformers/optimization.py:56: mem 0.000B:             0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
/home/patrick/python_bin/transformers/trainer.py:328: mem 0.000B:                     model.zero_grad()
/home/patrick/python_bin/transformers/trainer.py:329: mem 0.000B:                     global_step += 1
/home/patrick/python_bin/transformers/trainer.py:331: mem 0.000B:                     if self.args.local_rank in [-1, 0]:
/home/patrick/python_bin/transformers/trainer.py:332: mem 0.000B:                         if (self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0) or (
/home/patrick/python_bin/transformers/trainer.py:335: mem 0.000B:                             logs = {}
/home/patrick/python_bin/transformers/trainer.py:336: mem 0.000B:                             if self.args.evaluate_during_training:
/home/patrick/python_bin/transformers/trainer.py:342: mem 0.000B:                             loss_scalar = (tr_loss - logging_loss) / self.args.logging_steps
/home/patrick/python_bin/transformers/trainer.py:343: mem 0.000B:                             learning_rate_scalar = scheduler.get_last_lr()[0]
/home/patrick/python_bin/transformers/trainer.py:344: mem 0.000B:                             logs["learning_rate"] = learning_rate_scalar
/home/patrick/python_bin/transformers/trainer.py:345: mem 0.000B:                             logs["loss"] = loss_scalar
/home/patrick/python_bin/transformers/trainer.py:346: mem 0.000B:                             logging_loss = tr_loss
/home/patrick/python_bin/transformers/trainer.py:348: mem 0.000B:                             if self.tb_writer:
/home/patrick/python_bin/transformers/trainer.py:349: mem 0.000B:                                 for k, v in logs.items():
/home/patrick/python_bin/transformers/trainer.py:350: mem 0.000B:                                     self.tb_writer.add_scalar(k, v, global_step)
/home/patrick/python_bin/transformers/trainer.py:349: mem 0.000B:                                 for k, v in logs.items():
/home/patrick/python_bin/transformers/trainer.py:350: mem 0.000B:                                     self.tb_writer.add_scalar(k, v, global_step)
/home/patrick/python_bin/transformers/trainer.py:349: mem 0.000B:                                 for k, v in logs.items():
/home/patrick/python_bin/transformers/trainer.py:351: mem 260.000KB:                             epoch_iterator.write(json.dumps({**logs, **{"step": global_step}}))
/home/patrick/python_bin/transformers/trainer.py:353: mem 0.000B:                         if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:
/home/patrick/python_bin/transformers/trainer.py:368: mem 0.000B:                 if self.args.max_steps > 0 and global_step > self.args.max_steps:
/home/patrick/python_bin/transformers/trainer.py:307: mem 0.000B:             for step, inputs in enumerate(epoch_iterator):
/home/patrick/python_bin/transformers/trainer.py:371: mem 0.000B:             if self.args.max_steps > 0 and global_step > self.args.max_steps:
/home/patrick/python_bin/transformers/trainer.py:305: mem 12.000KB:         for epoch in train_iterator:
/home/patrick/python_bin/transformers/trainer.py:375: mem 0.000B:         if self.tb_writer:
/home/patrick/python_bin/transformers/trainer.py:376: mem 80.000KB:             self.tb_writer.close()
/home/patrick/python_bin/transformers/trainer.py:378: mem 0.000B:         logger.info("\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n")
/home/patrick/python_bin/transformers/trainer.py:379: mem 0.000B:         return TrainOutput(global_step, tr_loss / global_step)

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:854: mem 1.501GB:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:867: mem 768.000MB:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:818: mem 767.871MB:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 767.828MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:887: mem 767.766MB:         norm_attention_output = self.layer_norm(attention_output)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:921: mem 767.742MB:             attn_output = prev_attn_output + attn_output

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:443: mem -960.059MB:         return LSHSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs, buckets=buckets)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:839: mem -1.125GB:             hidden_states=attention_output, attention_probs=self_attention_outputs.attention_probs, buckets=buckets
=> /home/patrick/python_bin/transformers/trainer.py:400: mem -1.128GB:             loss.backward()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:622: mem -1.156GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:768: mem -1.781GB:         return LocalSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:890: mem -2.250GB:         return output

Total memory increase: 12.755GB
