Running with arguments Namespace(amp=False, average_over=30, batch_sizes=[1, 8], bert=False, csv_filename='results/results_1586175719.csv', csv_memory_filename='memory/memory_1586175719.csv', fp16=False, keras_predict=False, models=['gpt2', 'bert-base-cased', 'xlnet-base-cased', 'xlm-mlm-en-2048', 'transfo-xl-wt103', 'openai-gpt', 'distilbert-base-uncased', 'distilgpt2', 'roberta-base', 'ctrl'], no_memory=False, no_reformer=False, no_speed=False, num_hashes=4, save_to_csv=True, slice_sizes=[1024, 2048, 4096, 8192, 16384, 32768], tensorflow=False, torch=True, torch_cuda=True, torchscript=False, verbose=True, xla=False)
1 / 1

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 664.000KB:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 2.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 2.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 2.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 20.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 20.000MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 240.000KB:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -48.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 20.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 20.000MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 4.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 2.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 664.000KB:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -48.000MB:         return outputs

Total memory increase: 48.883MB
Going through model with sequence of shape torch.Size([1, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 2.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 312.000KB:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 20.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 16.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 20.000MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 20.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -16.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 4.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -88.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 20.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 20.000MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 20.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 16.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -16.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -88.000MB:         return outputs

Total memory increase: 124.305MB
Going through model with sequence of shape torch.Size([1, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 2.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem -4.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -16.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 36.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 32.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 32.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 16.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -66.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 2.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -100.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 36.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 32.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 32.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem -4.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -16.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -66.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -100.000MB:         return outputs

Total memory increase: 186.000MB
Going through model with sequence of shape torch.Size([1, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 32.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 32.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -64.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 64.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -146.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -158.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 64.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 40.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 34.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 32.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -64.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -146.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -158.000MB:         return outputs

Total memory increase: 368.000MB
Going through model with sequence of shape torch.Size([1, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 128.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 128.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 2.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 2.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 16.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 20.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 128.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 16.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -288.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 20.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -342.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 128.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 128.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 128.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -288.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -342.000MB:         return outputs

Total memory increase: 920.000MB
Going through model with sequence of shape torch.Size([1, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 20.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 512.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 20.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 34.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 2.000MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 32.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 32.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -576.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 20.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 48.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 20.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -694.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 512.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -576.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -694.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets

Total memory increase: 2.303GB
Going through model with sequence of shape torch.Size([1, 32768, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 20.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 64.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -146.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -158.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 64.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 40.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 34.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -146.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -158.000MB:         return outputs

Total memory increase: 324.000MB
Going through model with sequence of shape torch.Size([8, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 16.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 16.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 2.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 2.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 16.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 128.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 16.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 16.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -288.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 20.000MB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -338.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 128.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -288.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -338.000MB:         return outputs

Total memory increase: 692.000MB
Going through model with sequence of shape torch.Size([8, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 20.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 64.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 64.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 20.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 34.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 32.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 32.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -576.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 20.000MB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 48.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 20.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -692.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 64.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 64.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -576.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -692.000MB:         return outputs

Total memory increase: 1.426GB
Going through model with sequence of shape torch.Size([8, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 16.000MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 16.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 16.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 256.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 20.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 64.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 384.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 20.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 512.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 64.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 64.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.145GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 16.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 16.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -16.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 16.000MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 32.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 96.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 16.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -16.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.316GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.000MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 512.000MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 384.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 256.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -16.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -16.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.145GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.316GB:         return outputs

Total memory increase: 3.117GB
Going through model with sequence of shape torch.Size([8, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 1.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 1.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -2.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 128.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 128.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 128.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 128.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 128.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 128.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 128.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1.000GB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 128.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 128.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 32.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 324.000KB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 192.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 32.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 32.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -32.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.439GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 1.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 1.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1.000GB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -32.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -2.000GB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.439GB:         return outputs

Total memory increase: 6.971GB
Going through model with sequence of shape torch.Size([8, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 2.000MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 64.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 4.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 4.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -8.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 16.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 16.000MB:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 16.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem -16.000MB:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -16.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 256.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 256.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 256.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 256.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 256.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 256.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 256.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 256.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 256.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 1.500GB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 2.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 2.000GB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 1.000GB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 256.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 256.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -4.531GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 20.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 64.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -512.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 64.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -64.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 66.000MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 128.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 384.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 64.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -64.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -5.102GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 4.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 4.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 2.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 2.000GB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 1.500GB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 1.000GB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -64.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -64.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -512.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -4.531GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -5.102GB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -8.000GB:         return offset_buckets

Total memory increase: 18.289GB
Going through model with sequence of shape torch.Size([8, 32768, 64])
=========== RESULTS ===========
	======= MODEL CHECKPOINT: ReformerLayer =======
		===== BATCH SIZE: 1 =====
		ReformerLayer/1/1024: 0.002s 48.883MB
		ReformerLayer/1/2048: 0.002s 124.305MB
		ReformerLayer/1/4096: 0.003s 186.000MB
		ReformerLayer/1/8192: 0.005s 368.000MB
		ReformerLayer/1/16384: 0.01s 920.000MB
		ReformerLayer/1/32768: 0.023s 2.303GB
		===== BATCH SIZE: 8 =====
		ReformerLayer/8/1024: 0.004s 324.000MB
		ReformerLayer/8/2048: 0.008s 692.000MB
		ReformerLayer/8/4096: 0.015s 1.426GB
		ReformerLayer/8/8192: 0.032s 3.117GB
		ReformerLayer/8/16384: 0.07s 6.971GB
		ReformerLayer/8/32768: 0.175s 18.289GB
