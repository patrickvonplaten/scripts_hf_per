Running with arguments Namespace(amp=False, average_over=30, batch_sizes=[1, 8], bert=False, csv_filename='results/results_1586173415.csv', csv_memory_filename='memory/memory_1586173415.csv', fp16=False, keras_predict=False, models=['gpt2', 'bert-base-cased', 'xlnet-base-cased', 'xlm-mlm-en-2048', 'transfo-xl-wt103', 'openai-gpt', 'distilbert-base-uncased', 'distilgpt2', 'roberta-base', 'ctrl'], no_memory=False, no_reformer=False, no_speed=False, num_hashes=2, save_to_csv=True, slice_sizes=[1024, 2048, 4096, 8192, 16384, 32768], tensorflow=False, torch=True, torch_cuda=False, torchscript=False, verbose=True, xla=False)
1 / 1

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 6.020MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 1.805MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 264.000KB:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -11.871MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 6.020MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 1.805MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 264.000KB:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -11.871MB:         return outputs

Total memory increase: 8.082MB
Going through model with sequence of shape torch.Size([1, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 3.352MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 3.863MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 4.125MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 3.867MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 4.125MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 3.867MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 3.863MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 3.352MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Total memory increase: 15.207MB
Going through model with sequence of shape torch.Size([1, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 23.973MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 15.984MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 15.984MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 7.734MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -40.707MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 23.973MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 15.984MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 15.984MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 7.734MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -40.707MB:         return outputs

Total memory increase: 63.676MB
Going through model with sequence of shape torch.Size([1, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 31.965MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -31.984MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 31.965MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 31.969MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 13.664MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 15.984MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 15.984MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -63.750MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -35.871MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 31.969MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 31.965MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 31.965MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 15.984MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 15.984MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 13.664MB:         query_key_dots = query_key_dots - self_mask * 1e5

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -31.984MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -35.871MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -63.750MB:         return out_vectors, logits, dots

Total memory increase: 141.531MB
Going through model with sequence of shape torch.Size([1, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.902MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 63.934MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 280.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 31.980MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -128.008MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -64.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.902MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 63.934MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 31.980MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 280.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.004MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -128.008MB:         return out_vectors, logits, dots

Total memory increase: 319.988MB
Going through model with sequence of shape torch.Size([1, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 255.746MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.262MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 127.875MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -40.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 192.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 47.949MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.980MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.262MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 255.746MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 127.875MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -40.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.980MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.004MB:         return offset_buckets

Total memory increase: 944.008MB
Going through model with sequence of shape torch.Size([1, 32768, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
=> /home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
=> /home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Total memory increase: 0.000B
Going through model with sequence of shape torch.Size([8, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -20.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -214.004MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
=> /home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
=> /home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -20.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -214.004MB:         return outputs

Total memory increase: 31.965MB
Going through model with sequence of shape torch.Size([8, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.133MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem -8.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 224.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -72.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 63.934MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.973MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 128.133MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/activations.py:18: mem 63.934MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 224.000KB:         query_key_dots = query_key_dots - self_mask * 1e5

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem -8.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -72.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.973MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots

Total memory increase: 448.023MB
Going through model with sequence of shape torch.Size([8, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 128.129MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.871MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 63.934MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 31.969MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.180MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 191.809MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.211MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 60.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 255.855MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 60.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 128.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 31.965MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 31.969MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -575.957MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem -20.000KB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 31.965MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 96.191MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -576.039MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 256.211MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 255.855MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 191.809MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 128.129MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.871MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:123: mem -20.000KB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.004MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -575.957MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -576.039MB:         return outputs

Total memory increase: 1.375GB
Going through model with sequence of shape torch.Size([8, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 511.988MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.016MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 63.934MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 63.938MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 63.938MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 63.938MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 168.000KB:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 63.934MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.129MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.129MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 383.879MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.164MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem -132.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 511.973MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 132.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.004MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 63.934MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 63.938MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.125GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 31.949MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem -20.000KB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.008MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 31.965MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 31.969MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -31.969MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 32.223MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 63.938MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 191.887MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 31.965MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 32.227MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -31.969MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.469GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.164MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.016MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 511.988MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 511.973MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 383.879MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.004MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -31.969MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -31.969MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.008MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.125GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.469GB:         return outputs

Total memory increase: 3.750GB
Going through model with sequence of shape torch.Size([8, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 63.934MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 2.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -4.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 7.730MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 128.133MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 128.133MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 127.875MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 128.133MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem -56.000KB:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 127.871MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 264.000KB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 127.871MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.160MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 127.871MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.160MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 767.762MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 15.984MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 12.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1023.949MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 12.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 8.133MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.012MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 127.871MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 127.875MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 63.902MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 228.000KB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.008MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 63.934MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 63.938MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -63.938MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 63.934MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 127.875MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 384.293MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 63.934MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 63.938MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -63.938MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.984GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 2.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1023.949MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 767.762MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.012MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -63.938MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -63.938MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.008MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.984GB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -4.000GB:         return offset_buckets

Total memory increase: 9.593GB
Going through model with sequence of shape torch.Size([8, 32768, 64])
=========== RESULTS ===========
	======= MODEL CHECKPOINT: ReformerLayer =======
		===== BATCH SIZE: 1 =====
		ReformerLayer/1/1024: 0.028s 8.082MB
		ReformerLayer/1/2048: 0.058s 15.207MB
		ReformerLayer/1/4096: 0.124s 63.676MB
		ReformerLayer/1/8192: 0.218s 141.531MB
		ReformerLayer/1/16384: 0.597s 319.988MB
		ReformerLayer/1/32768: 1.487s 944.008MB
		===== BATCH SIZE: 8 =====
		ReformerLayer/8/1024: 0.188s 0.000B
		ReformerLayer/8/2048: 0.486s 31.965MB
		ReformerLayer/8/4096: 1.11s 448.023MB
		ReformerLayer/8/8192: 2.548s 1.375GB
		ReformerLayer/8/16384: 5.965s 3.750GB
		ReformerLayer/8/32768: 13.583s 9.593GB
