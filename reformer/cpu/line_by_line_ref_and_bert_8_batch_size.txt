Running with arguments Namespace(amp=False, average_over=30, batch_sizes=[1, 8], csv_filename='results/results_1586167677.csv', csv_memory_filename='memory/memory_1586167677.csv', fp16=False, keras_predict=False, models=['gpt2', 'bert-base-cased', 'xlnet-base-cased', 'xlm-mlm-en-2048', 'transfo-xl-wt103', 'openai-gpt', 'distilbert-base-uncased', 'distilgpt2', 'roberta-base', 'ctrl'], no_memory=False, no_speed=False, save_to_csv=True, slice_sizes=[1024, 2048, 4096, 8192, 16384, 32768], tensorflow=False, torch=True, torch_cuda=False, torchscript=False, verbose=True, xla=False)
1 / 2

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 3.910MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 2.062MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 1.805MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 2.062MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 3.910MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 2.062MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 2.062MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 1.805MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Total memory increase: 9.840MB
Going through model with sequence of shape torch.Size([1, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 2.574MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 8.250MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 11.859MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 8.250MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 11.859MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 8.250MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 8.250MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 2.574MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Total memory increase: 30.934MB
Going through model with sequence of shape torch.Size([1, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1.805MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 7.988MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -34.234MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 7.988MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1.805MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
=> /home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -34.234MB:         return outputs

Total memory increase: 9.793MB
Going through model with sequence of shape torch.Size([1, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 13.660MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 31.969MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 15.984MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 15.984MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 15.984MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -31.754MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 31.969MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 15.984MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 15.984MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 15.984MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 13.660MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -31.754MB:         return out_vectors, logits, dots

Total memory increase: 93.582MB
Going through model with sequence of shape torch.Size([1, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.902MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.191MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 16.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 31.980MB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -20.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -127.988MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -64.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 127.902MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 64.191MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 31.980MB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 16.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -20.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -127.988MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -128.004MB:         return offset_buckets

Total memory increase: 319.988MB
Going through model with sequence of shape torch.Size([1, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 256.004MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.004MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 127.875MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 224.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -72.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 31.965MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.977MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 256.004MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 256.004MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 127.875MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -72.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.977MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -256.008MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -512.004MB:         return offset_buckets

Total memory increase: 928.055MB
Going through model with sequence of shape torch.Size([1, 32768, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 0.000B:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 0.000B:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 0.000B:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
=> /home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
=> /home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem 0.000B:         return outputs

Total memory increase: 0.000B
Going through model with sequence of shape torch.Size([8, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 63.934MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 32.238MB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -24.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -128.008MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -101.883MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 63.934MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 63.934MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 32.238MB:             query_key_dots = query_key_dots - causal_mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 31.965MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -24.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:241: mem -40.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -101.883MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -128.008MB:         return out_vectors, logits, dots

Total memory increase: 192.070MB
Going through model with sequence of shape torch.Size([8, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 143.859MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 264.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -40.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 192.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 15.980MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -255.988MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 15.980MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 79.922MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.957MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 143.859MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 127.871MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/activations.py:18: mem 79.922MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 63.934MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 63.934MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 15.980MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -40.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -127.957MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -255.988MB:         return out_vectors, logits, dots

Total memory increase: 511.926MB
Going through model with sequence of shape torch.Size([8, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 255.809MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.004MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 63.934MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 31.969MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 32.180MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.066MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 255.953MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 60.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.113MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -204.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 128.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 31.965MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 31.969MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -575.957MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem -20.000KB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 32.223MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 95.934MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -576.039MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 256.113MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 255.953MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 255.809MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.066MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 127.871MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 96.113MB:         return torch.cat(slices, dim=3)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:123: mem -20.000KB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -204.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -256.004MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -575.957MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -576.039MB:         return outputs

Total memory increase: 1.375GB
Going through model with sequence of shape torch.Size([8, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 511.754MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.270MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 63.934MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 63.938MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 68.000KB:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 64.191MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 63.676MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.129MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 63.934MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 64.129MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 383.879MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.164MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 132.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 511.973MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem -132.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.004MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 63.934MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 63.938MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.125GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 32.207MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem -20.000KB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.008MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 31.965MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 31.969MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -31.969MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 31.965MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 63.938MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 192.145MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 31.965MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 31.969MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -31.969MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.313GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 512.270MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 512.164MB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 511.973MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 511.754MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 383.879MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 256.004MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -31.969MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -31.969MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.008MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -1.000GB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -1.125GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -1.313GB:         return outputs

Total memory increase: 3.625GB
Going through model with sequence of shape torch.Size([8, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:430: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:358: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:359: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 63.934MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:300: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:301: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 2.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -4.000GB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 128.129MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 127.875MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:295: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:296: mem 127.875MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 128.133MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:285: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:290: mem 64.000KB:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:291: mem 127.871MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:286: mem 264.000KB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 127.871MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.160MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 127.613MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 128.160MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.020MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:273: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:276: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:277: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:282: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 12.000KB:             query_key_dots = query_key_dots - causal_mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1023.949MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
/home/patrick/python_bin/transformers/modeling_reformer.py:226: mem 12.000KB:         query_key_dots = query_key_dots - self_mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:241: mem 8.000KB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.012MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:245: mem 0.000B:         dots = self.dropout(dots)
/home/patrick/python_bin/transformers/modeling_reformer.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:252: mem 127.871MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:255: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:256: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:320: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:321: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:324: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:259: mem 127.875MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:313: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:309: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:310: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 64.160MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:305: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:306: mem -36.000KB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.008MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:365: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:338: mem 63.934MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:339: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 63.938MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -63.938MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:366: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:431: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:408: mem 63.934MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:409: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:381: mem 128.133MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:382: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 384.035MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:394: mem 63.934MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:395: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 63.938MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -63.938MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.938GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 2.000GB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000GB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:221: mem 1.000GB:             causal_mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:225: mem 1023.949MB:         self_mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long()
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.020MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:242: mem 512.012MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:342: mem -63.938MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem -63.938MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.008MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:262: mem -2.250GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:435: mem -2.938GB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -4.000GB:         return offset_buckets

Total memory increase: 9.562GB
Going through model with sequence of shape torch.Size([8, 32768, 64])
2 / 2

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
/home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
/home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:227: mem 0.000B:             mixed_value_layer = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:229: mem 0.000B:         query_layer = self.transpose_for_scores(mixed_query_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:230: mem 0.000B:         key_layer = self.transpose_for_scores(mixed_key_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:231: mem 0.000B:         value_layer = self.transpose_for_scores(mixed_value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:234: mem 0.000B:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_bert.py:235: mem 0.000B:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:236: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:241: mem 0.000B:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
/home/patrick/python_bin/transformers/modeling_bert.py:245: mem 0.000B:         attention_probs = self.dropout(attention_probs)
/home/patrick/python_bin/transformers/modeling_bert.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:251: mem 0.000B:         context_layer = torch.matmul(attention_probs, value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:253: mem 0.000B:         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_bert.py:254: mem 0.000B:         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
/home/patrick/python_bin/transformers/modeling_bert.py:255: mem 0.000B:         context_layer = context_layer.view(*new_context_layer_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:257: mem 0.000B:         outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
/home/patrick/python_bin/transformers/modeling_bert.py:258: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:316: mem 0.000B:         attention_output = self.output(self_outputs[0], hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:269: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:270: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:271: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:272: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:317: mem 0.000B:         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_bert.py:318: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:369: mem 0.000B:         attention_output = self_attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_bert.py:370: mem 0.000B:         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights
/home/patrick/python_bin/transformers/modeling_bert.py:372: mem 0.000B:         if self.is_decoder and encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:379: mem 0.000B:         intermediate_output = self.intermediate(attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:331: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:332: mem 0.000B:         hidden_states = self.intermediate_act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_bert.py:333: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:380: mem 0.000B:         layer_output = self.output(intermediate_output, attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
/home/patrick/python_bin/transformers/modeling_bert.py:382: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
=> /home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
=> /home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
=> /home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
=> /home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
=> /home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
=> /home/patrick/python_bin/transformers/modeling_bert.py:382: mem 0.000B:         return outputs

Total memory increase: 0.000B
Going through model with sequence of shape torch.Size([1, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
/home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
/home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:227: mem 0.000B:             mixed_value_layer = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:229: mem 0.000B:         query_layer = self.transpose_for_scores(mixed_query_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:230: mem 0.000B:         key_layer = self.transpose_for_scores(mixed_key_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:231: mem 0.000B:         value_layer = self.transpose_for_scores(mixed_value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:234: mem 0.000B:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_bert.py:235: mem 0.000B:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:236: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:241: mem 0.000B:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
/home/patrick/python_bin/transformers/modeling_bert.py:245: mem 0.000B:         attention_probs = self.dropout(attention_probs)
/home/patrick/python_bin/transformers/modeling_bert.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:251: mem 0.000B:         context_layer = torch.matmul(attention_probs, value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:253: mem 0.000B:         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_bert.py:254: mem 0.000B:         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
/home/patrick/python_bin/transformers/modeling_bert.py:255: mem 0.000B:         context_layer = context_layer.view(*new_context_layer_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:257: mem 0.000B:         outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
/home/patrick/python_bin/transformers/modeling_bert.py:258: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:316: mem 0.000B:         attention_output = self.output(self_outputs[0], hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:269: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:270: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:271: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:272: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:317: mem 0.000B:         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_bert.py:318: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:369: mem 0.000B:         attention_output = self_attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_bert.py:370: mem 0.000B:         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights
/home/patrick/python_bin/transformers/modeling_bert.py:372: mem 0.000B:         if self.is_decoder and encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:379: mem 0.000B:         intermediate_output = self.intermediate(attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:331: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:332: mem 0.000B:         hidden_states = self.intermediate_act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_bert.py:333: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:380: mem 0.000B:         layer_output = self.output(intermediate_output, attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
/home/patrick/python_bin/transformers/modeling_bert.py:382: mem 0.000B:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
=> /home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
=> /home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
=> /home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
=> /home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
=> /home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
=> /home/patrick/python_bin/transformers/modeling_bert.py:382: mem 0.000B:         return outputs

Total memory increase: 0.000B
Going through model with sequence of shape torch.Size([1, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
/home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
/home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:227: mem 0.000B:             mixed_value_layer = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:229: mem 0.000B:         query_layer = self.transpose_for_scores(mixed_query_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:230: mem 0.000B:         key_layer = self.transpose_for_scores(mixed_key_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:231: mem 0.000B:         value_layer = self.transpose_for_scores(mixed_value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:234: mem 127.871MB:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_bert.py:235: mem 264.000KB:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:236: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:241: mem 127.871MB:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
/home/patrick/python_bin/transformers/modeling_bert.py:245: mem 0.000B:         attention_probs = self.dropout(attention_probs)
/home/patrick/python_bin/transformers/modeling_bert.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:251: mem 0.000B:         context_layer = torch.matmul(attention_probs, value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:253: mem 0.000B:         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_bert.py:254: mem 0.000B:         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
/home/patrick/python_bin/transformers/modeling_bert.py:255: mem 0.000B:         context_layer = context_layer.view(*new_context_layer_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:257: mem 0.000B:         outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
/home/patrick/python_bin/transformers/modeling_bert.py:258: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:316: mem 0.000B:         attention_output = self.output(self_outputs[0], hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:269: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:270: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:271: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:272: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:317: mem 0.000B:         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_bert.py:318: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:369: mem 0.000B:         attention_output = self_attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_bert.py:370: mem 0.000B:         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights
/home/patrick/python_bin/transformers/modeling_bert.py:372: mem 0.000B:         if self.is_decoder and encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:379: mem 0.000B:         intermediate_output = self.intermediate(attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:331: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:332: mem 0.000B:         hidden_states = self.intermediate_act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_bert.py:333: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:380: mem 0.000B:         layer_output = self.output(intermediate_output, attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
/home/patrick/python_bin/transformers/modeling_bert.py:382: mem -255.941MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:234: mem 127.871MB:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_bert.py:241: mem 127.871MB:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
=> /home/patrick/python_bin/transformers/modeling_bert.py:235: mem 264.000KB:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
=> /home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
=> /home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
=> /home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
=> /home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
=> /home/patrick/python_bin/transformers/modeling_bert.py:382: mem -255.941MB:         return outputs

Total memory increase: 256.000MB
Going through model with sequence of shape torch.Size([1, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
/home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
/home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:227: mem 0.000B:             mixed_value_layer = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:229: mem 0.000B:         query_layer = self.transpose_for_scores(mixed_query_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:230: mem 0.000B:         key_layer = self.transpose_for_scores(mixed_key_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:231: mem 0.000B:         value_layer = self.transpose_for_scores(mixed_value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:234: mem 511.754MB:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_bert.py:235: mem 264.000KB:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:236: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:241: mem 511.754MB:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
/home/patrick/python_bin/transformers/modeling_bert.py:245: mem 0.000B:         attention_probs = self.dropout(attention_probs)
/home/patrick/python_bin/transformers/modeling_bert.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:251: mem 0.000B:         context_layer = torch.matmul(attention_probs, value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:253: mem 0.000B:         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_bert.py:254: mem 0.000B:         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
/home/patrick/python_bin/transformers/modeling_bert.py:255: mem 0.000B:         context_layer = context_layer.view(*new_context_layer_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:257: mem 0.000B:         outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
/home/patrick/python_bin/transformers/modeling_bert.py:258: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:316: mem 0.000B:         attention_output = self.output(self_outputs[0], hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:269: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:270: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:271: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:272: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:317: mem 0.000B:         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_bert.py:318: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:369: mem 0.000B:         attention_output = self_attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_bert.py:370: mem 0.000B:         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights
/home/patrick/python_bin/transformers/modeling_bert.py:372: mem 0.000B:         if self.is_decoder and encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:379: mem 0.000B:         intermediate_output = self.intermediate(attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:331: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:332: mem 0.000B:         hidden_states = self.intermediate_act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_bert.py:333: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:380: mem 0.000B:         layer_output = self.output(intermediate_output, attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
/home/patrick/python_bin/transformers/modeling_bert.py:382: mem -1023.754MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:234: mem 511.754MB:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_bert.py:241: mem 511.754MB:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
=> /home/patrick/python_bin/transformers/modeling_bert.py:235: mem 264.000KB:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
=> /home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
=> /home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
=> /home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
=> /home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
=> /home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
=> /home/patrick/python_bin/transformers/modeling_bert.py:382: mem -1023.754MB:         return outputs

Total memory increase: 1023.766MB
Going through model with sequence of shape torch.Size([1, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_bert.py:368: mem 0.000B:         self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_bert.py:313: mem 0.000B:         self_outputs = self.self(
/home/patrick/python_bin/transformers/modeling_bert.py:314: mem 0.000B:             hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
/home/patrick/python_bin/transformers/modeling_bert.py:216: mem 0.000B:         mixed_query_layer = self.query(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:221: mem 0.000B:         if encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:226: mem 0.000B:             mixed_key_layer = self.key(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:227: mem 0.000B:             mixed_value_layer = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:229: mem 0.000B:         query_layer = self.transpose_for_scores(mixed_query_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:230: mem 0.000B:         key_layer = self.transpose_for_scores(mixed_key_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:231: mem 0.000B:         value_layer = self.transpose_for_scores(mixed_value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:204: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:205: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:206: mem 0.000B:         return x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_bert.py:234: mem 2.000GB:         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_bert.py:235: mem 264.000KB:         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_bert.py:236: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:241: mem 2.000GB:         attention_probs = nn.Softmax(dim=-1)(attention_scores)
/home/patrick/python_bin/transformers/modeling_bert.py:245: mem 0.000B:         attention_probs = self.dropout(attention_probs)
/home/patrick/python_bin/transformers/modeling_bert.py:248: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:251: mem 0.000B:         context_layer = torch.matmul(attention_probs, value_layer)
/home/patrick/python_bin/transformers/modeling_bert.py:253: mem 0.000B:         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_bert.py:254: mem 0.000B:         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
/home/patrick/python_bin/transformers/modeling_bert.py:255: mem 0.000B:         context_layer = context_layer.view(*new_context_layer_shape)
/home/patrick/python_bin/transformers/modeling_bert.py:257: mem 0.000B:         outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
/home/patrick/python_bin/transformers/modeling_bert.py:258: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:316: mem 0.000B:         attention_output = self.output(self_outputs[0], hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:269: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:270: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:271: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:272: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:317: mem 0.000B:         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_bert.py:318: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_bert.py:369: mem 0.000B:         attention_output = self_attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_bert.py:370: mem 0.000B:         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights
/home/patrick/python_bin/transformers/modeling_bert.py:372: mem 0.000B:         if self.is_decoder and encoder_hidden_states is not None:
/home/patrick/python_bin/transformers/modeling_bert.py:379: mem 0.000B:         intermediate_output = self.intermediate(attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:331: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:332: mem 0.000B:         hidden_states = self.intermediate_act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_bert.py:333: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:380: mem 0.000B:         layer_output = self.output(intermediate_output, attention_output)
/home/patrick/python_bin/transformers/modeling_bert.py:344: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:345: mem 0.000B:         hidden_states = self.dropout(hidden_states)
/home/patrick/python_bin/transformers/modeling_bert.py:346: mem 0.000B:         hidden_states = self.LayerNorm(hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_bert.py:347: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_bert.py:381: mem 0.000B:         outputs = (layer_output,) + outputs
/home/patrick/python_bin/transformers/modeling_bert.py:382: mem -4.000GB:         return outputs