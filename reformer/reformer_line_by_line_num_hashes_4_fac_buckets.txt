Running with arguments Namespace(amp=False, average_over=30, batch_sizes=[1, 8], bert=False, csv_filename='results/results_1586249498.csv', csv_memory_filename='memory/memory_1586249498.csv', fp16=False, keras_predict=False, models=['gpt2', 'bert-base-cased', 'xlnet-base-cased', 'xlm-mlm-en-2048', 'transfo-xl-wt103', 'openai-gpt', 'distilbert-base-uncased', 'distilgpt2', 'roberta-base', 'ctrl'], no_memory=False, no_reformer=False, no_speed=False, num_hashes=4, save_to_csv=True, slice_sizes=[1024, 2048, 4096, 8192, 16384, 32768], tensorflow=False, torch=True, torch_cuda=True, torchscript=False, verbose=True, xla=False)
1 / 1

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 724.000KB:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 2.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 2.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 2.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 20.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 0.000B:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 0.000B:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 20.000MB:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 0.000B:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 188.000KB:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -48.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 20.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 20.000MB:             query_key_dots = query_key_dots - mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 4.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 2.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 2.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 724.000KB:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -48.000MB:         return outputs

Total memory increase: 48.891MB
Going through model with sequence of shape torch.Size([1, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 2.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 0.000B:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 312.000KB:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 20.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 16.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 20.000MB:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem -16.000MB:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 20.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem 0.000B:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 4.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -88.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 20.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 20.000MB:             query_key_dots = query_key_dots - mask * 1e9
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 20.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 16.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:227: mem -16.000MB:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -88.000MB:         return outputs

Total memory increase: 124.305MB
Going through model with sequence of shape torch.Size([1, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 2.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -20.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem 0.000B:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 0.000B:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 36.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 32.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 16.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -34.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 2.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 2.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -100.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 36.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 32.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 20.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 16.000MB:         dots = torch.exp(query_key_dots - logits)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -20.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -34.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -100.000MB:         return outputs

Total memory increase: 154.000MB
Going through model with sequence of shape torch.Size([1, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 20.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 64.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -82.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -158.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 64.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 40.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 34.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 20.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -82.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -158.000MB:         return outputs

Total memory increase: 260.000MB
Going through model with sequence of shape torch.Size([1, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 20.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -4.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 4.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 2.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 2.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 16.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 20.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 128.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 16.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -160.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 20.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -342.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 128.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 52.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -160.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -342.000MB:         return outputs

Total memory increase: 560.000MB
Going through model with sequence of shape torch.Size([1, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 20.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 32.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 20.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 34.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 2.000MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 32.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 32.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -320.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 20.000MB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 48.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 20.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -694.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -320.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -694.000MB:         return outputs

Total memory increase: 1.084GB
Going through model with sequence of shape torch.Size([1, 32768, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 0.000B:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 20.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 20.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 0.000B:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 20.000MB:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 2.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 64.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 0.000B:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 0.000B:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -82.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 0.000B:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -158.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 64.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 48.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 40.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 34.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 32.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 20.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -82.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -158.000MB:         return outputs

Total memory increase: 260.000MB
Going through model with sequence of shape torch.Size([8, 1024, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 0.000B:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 20.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:138: mem 0.000B:             assert self.num_buckets % 2 == 0, 'There should be an even number of bucktes, but `self.num_bucktes`: {}'.format(self.num_buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:139: mem 0.000B:             rotation_size = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:140: mem 0.000B:             num_buckets = self.num_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 16.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:164: mem 16.000MB:             rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:165: mem 0.000B:             buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 2.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 2.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 2.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 0.000B:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 16.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 16.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 16.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 128.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 16.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 16.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -160.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 20.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -342.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 128.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 96.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 64.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/activations.py:18: mem 40.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -2.000MB:         return sorted_ticker, undo_sorted_ticker
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -32.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -32.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -160.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -342.000MB:         return outputs

Total memory increase: 568.000MB
Going through model with sequence of shape torch.Size([8, 2048, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 0.000B:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 20.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 0.000B:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 18.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 14.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 0.000B:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -12.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 20.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 32.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 32.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 34.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 32.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 32.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 32.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 32.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -320.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 20.000MB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 48.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 20.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 0.000B:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -692.000MB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 256.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 192.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 128.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -20.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -64.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -320.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -692.000MB:         return outputs

Total memory increase: 1.082GB
Going through model with sequence of shape torch.Size([8, 4096, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 16.000MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 16.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 16.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 40.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 24.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 20.000MB:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -48.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -16.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 0.000B:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 0.000B:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 20.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 64.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 64.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 64.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 64.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 64.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 64.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 384.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 0.000B:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 512.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 256.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 64.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 64.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -640.000MB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 0.000B:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 0.000B:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 16.000MB:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 0.000B:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 16.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem 256.000KB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 0.000B:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 32.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 96.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 16.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 16.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -16.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -1.320GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 512.000MB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 384.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 256.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 128.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 128.000MB:         return torch.gather(vectors, 2, expanded_idxs)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 128.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -16.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -16.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -24.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -128.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -640.000MB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -1.320GB:         return outputs

Total memory increase: 2.149GB
Going through model with sequence of shape torch.Size([8, 8192, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 32.000MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 32.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 32.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 96.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 32.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 20.000MB:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem -64.000MB:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 0.000B:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -64.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 0.000B:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 20.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 20.000MB:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 0.000B:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem 0.000B:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 128.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 128.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 128.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 128.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 0.000B:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 128.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem 0.000B:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 128.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 128.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 128.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 128.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 0.000B:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 1.000GB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 0.000B:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 512.000MB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 128.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 128.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -1.266GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 2.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 32.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 32.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 32.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem -32.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 32.000MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 192.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 32.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 32.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -32.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -2.654GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 1.000GB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 768.000MB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 512.000MB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 272.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 256.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 256.000MB:         return torch.gather(vectors, 2, expanded_idxs)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:344: mem -32.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -32.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -64.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -256.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -1.266GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -2.654GB:         return outputs

Total memory increase: 4.357GB
Going through model with sequence of shape torch.Size([8, 16384, 64])

Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_reformer.py:432: mem 0.000B:         attention_outputs = self.attention(prev_attention_output, hidden_states, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:360: mem 66.000MB:         norm_hidden_states = self.layer_norm(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:361: mem 0.000B:         self_attention_outputs = self.self_attention(
/home/patrick/python_bin/transformers/modeling_reformer.py:362: mem 0.000B:             norm_hidden_states, head_mask
/home/patrick/python_bin/transformers/modeling_reformer.py:83: mem 0.000B:         sequence_length = hidden_states.shape[1]
/home/patrick/python_bin/transformers/modeling_reformer.py:84: mem 0.000B:         batch_size = hidden_states.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:86: mem 64.000MB:         mixed_query_key_vectors = self.query_key(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:87: mem 64.000MB:         mixed_value_vectors = self.value(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:89: mem 0.000B:         query_key_vectors = self._transpose_for_scores(mixed_query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:90: mem 0.000B:         value_vectors = self._transpose_for_scores(mixed_value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:302: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:303: mem 0.000B:         x = x.view(*new_x_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:304: mem 0.000B:         return x.transpose(2, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:92: mem 0.000B:         assert query_key_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:93: mem 0.000B:         assert value_vectors.shape[-1] == self.attention_head_size
/home/patrick/python_bin/transformers/modeling_reformer.py:96: mem 0.000B:         buckets = self._hash_vectors(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:132: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:137: mem 0.000B:         if isinstance(self.num_buckets, int):
/home/patrick/python_bin/transformers/modeling_reformer.py:143: mem 0.000B:             rotation_size, num_buckets = 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:145: mem 0.000B:                 assert num_bucket % 2 == 0, 'The number of buckets should be even, but `num_bucket`: {}'.format(num_bucket)
/home/patrick/python_bin/transformers/modeling_reformer.py:146: mem 0.000B:                 rotation_size += num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:147: mem 0.000B:                 num_buckets *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:144: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:149: mem 0.000B:         rotations_shape = (vectors.shape[-1], self.num_hashes, rotation_size // 2)
/home/patrick/python_bin/transformers/modeling_reformer.py:154: mem 0.000B:         numpy.random.seed(self.hash_seed)
/home/patrick/python_bin/transformers/modeling_reformer.py:155: mem 0.000B:         random_rotations = torch.tensor(numpy.random.normal(size=rotations_shape), dtype=torch.float32, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:161: mem 256.000MB:         rotated_vectors = torch.einsum('bmtd,dhr->bmhtr', vectors, random_rotations)
/home/patrick/python_bin/transformers/modeling_reformer.py:163: mem 0.000B:         if isinstance(self.num_buckets, int) or len(self.num_buckets) == 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:168: mem 0.000B:             buckets, cur_sum, cur_product = None, 0, 1
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:175: mem 16.000MB:                     buckets = torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:170: mem 0.000B:                 rotated_vectors = rotated_vectors[..., cur_sum:cur_sum + (num_bucket // 2)]
/home/patrick/python_bin/transformers/modeling_reformer.py:171: mem 0.000B:                 cur_sum += num_bucket // 2
/home/patrick/python_bin/transformers/modeling_reformer.py:172: mem 0.000B:                 rotated_vectors = torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:174: mem 0.000B:                 if buckets is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:177: mem 0.000B:                     buckets += cur_product * torch.argmax(rotated_vectors, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:179: mem 0.000B:                 cur_product *= num_bucket
/home/patrick/python_bin/transformers/modeling_reformer.py:169: mem 0.000B:             for num_bucket in self.num_buckets:
/home/patrick/python_bin/transformers/modeling_reformer.py:183: mem 0.000B:         offsets = torch.arange(self.num_hashes, device=vectors.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:184: mem 0.000B:         offsets = torch.reshape(offsets * num_buckets, (-1, 1))
/home/patrick/python_bin/transformers/modeling_reformer.py:187: mem 0.000B:         offsets = offsets.repeat(batch_size, self.num_attention_heads, 1, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:188: mem 16.000MB:         offset_buckets = self._merge_by_middle_dim(buckets + offsets)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:327: mem 0.000B:         elif len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:328: mem 0.000B:             return torch.reshape(vectors, new_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -272.000MB:         return offset_buckets
/home/patrick/python_bin/transformers/modeling_reformer.py:97: mem 0.000B:         assert int(buckets.shape[-1]) == self.num_hashes * sequence_length
/home/patrick/python_bin/transformers/modeling_reformer.py:99: mem 0.000B:         ticker, undo_ticker = self._get_ticker_and_undo_ticker(sequence_length, buckets)
/home/patrick/python_bin/transformers/modeling_reformer.py:193: mem 0.000B:         batch_size = buckets.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:196: mem 0.000B:         ticker = torch.arange(self.num_hashes * sequence_length, device=buckets.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:197: mem 16.000MB:         ticker = ticker.repeat(batch_size, self.num_attention_heads, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:199: mem 16.000MB:         buckets_and_t = sequence_length * buckets + (ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:202: mem 16.000MB:         sorted_ticker = torch.argsort(buckets_and_t, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:203: mem 16.000MB:         undo_sorted_ticker = torch.argsort(sorted_ticker, dim=-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:205: mem 0.000B:         sorted_ticker = (sorted_ticker % sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:206: mem -32.000MB:         return sorted_ticker, undo_sorted_ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:101: mem 0.000B:         query_key_vectors = self._gather_by_expansion(query_key_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 256.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 256.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:102: mem 0.000B:         value_vectors = self._gather_by_expansion(value_vectors, ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:297: mem 0.000B:         expanded_idxs = idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 256.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)
/home/patrick/python_bin/transformers/modeling_reformer.py:299: mem 256.000MB:         return torch.gather(vectors, 2, expanded_idxs)
/home/patrick/python_bin/transformers/modeling_reformer.py:106: mem 0.000B:         query_key_vectors = self._split_dim_by(query_key_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:107: mem 0.000B:         value_vectors = self._split_dim_by(value_vectors, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:108: mem 0.000B:         ticker = self._split_dim_by(ticker, -1, self.chunk_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:111: mem 0.000B:         if self.chunk_length is None:
/home/patrick/python_bin/transformers/modeling_reformer.py:114: mem 0.000B:         key_vectors = self._len_and_dim_norm(query_key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:287: mem 0.000B:         vectors = self._len_norm(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:292: mem 20.000MB:         variance = torch.mean(x**2, -1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:293: mem 276.000MB:         norm_x = x / torch.sqrt(variance + epsilon)
/home/patrick/python_bin/transformers/modeling_reformer.py:294: mem -20.000MB:         return norm_x
/home/patrick/python_bin/transformers/modeling_reformer.py:288: mem 0.000B:         vectors = vectors / torch.sqrt(torch.tensor(self.attention_head_size, device=vectors.device, dtype=torch.float32))
/home/patrick/python_bin/transformers/modeling_reformer.py:289: mem 0.000B:         return vectors
/home/patrick/python_bin/transformers/modeling_reformer.py:116: mem 0.000B:         out_vectors, logits, attention_probs = self._attend(query_key_vectors, key_vectors, value_vectors, ticker, undo_ticker, head_mask)
/home/patrick/python_bin/transformers/modeling_reformer.py:209: mem 0.000B:         key_vectors = self._look_adjacent(key_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 256.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 256.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:210: mem 0.000B:         value_vectors = self._look_adjacent(value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 256.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 256.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 1.500GB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
/home/patrick/python_bin/transformers/modeling_reformer.py:216: mem 0.000B:         query_info = ticker
/home/patrick/python_bin/transformers/modeling_reformer.py:217: mem 0.000B:         key_value_info = self._look_adjacent(ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:275: mem 0.000B:         if self.num_chunks_before == 0 and self.num_chunks_after == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:278: mem 0.000B:         slices = []
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 16.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:280: mem 0.000B:             if i == 0:
/home/patrick/python_bin/transformers/modeling_reformer.py:281: mem 0.000B:                 slices.append(vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:279: mem 0.000B:         for i in range(-self.num_chunks_before, self.num_chunks_after + 1):
/home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 16.000MB:         return torch.cat(slices, dim=3)
/home/patrick/python_bin/transformers/modeling_reformer.py:220: mem 0.000B:         if self.is_decoder:
/home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 2.000GB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:223: mem 0.000B:             query_key_dots = query_key_dots - mask * 1e9
/home/patrick/python_bin/transformers/modeling_reformer.py:227: mem 0.000B:         mask = torch.eq(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
/home/patrick/python_bin/transformers/modeling_reformer.py:228: mem 0.000B:         query_key_dots = query_key_dots - mask * 1e5
/home/patrick/python_bin/transformers/modeling_reformer.py:243: mem 16.000MB:         logits = torch.logsumexp(query_key_dots, dim=-1, keepdim=True)
/home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 1.000GB:         dots = torch.exp(query_key_dots - logits)
/home/patrick/python_bin/transformers/modeling_reformer.py:247: mem 0.000B:         dots = nn.functional.dropout(dots, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:250: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_reformer.py:254: mem 256.000MB:         out_vectors = torch.matmul(dots, value_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:257: mem 0.000B:         logits = self._merge_by_middle_dim(logits).squeeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:258: mem 0.000B:         out_vectors = self._merge_by_middle_dim(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:322: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:323: mem 0.000B:         new_dim_shape = (batch_size, self.num_attention_heads, -1)
/home/patrick/python_bin/transformers/modeling_reformer.py:325: mem 0.000B:         if len(vectors.shape) == 5:
/home/patrick/python_bin/transformers/modeling_reformer.py:326: mem 0.000B:             return torch.reshape(vectors, new_dim_shape + (vectors.shape[-1],))
/home/patrick/python_bin/transformers/modeling_reformer.py:260: mem 0.000B:         expanded_undo_sort_indices = undo_ticker.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:261: mem 256.000MB:         out_vectors = torch.gather(out_vectors, 2, expanded_undo_sort_indices)
/home/patrick/python_bin/transformers/modeling_reformer.py:262: mem 0.000B:         logits = torch.gather(logits, 2, undo_ticker)
/home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -2.531GB:         return out_vectors, logits, dots
/home/patrick/python_bin/transformers/modeling_reformer.py:118: mem 0.000B:         if self.num_hashes > 1:
/home/patrick/python_bin/transformers/modeling_reformer.py:119: mem 0.000B:             out_vectors = self._split_dim_by(out_vectors, self.num_hashes, sequence_length)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:315: mem 0.000B:             return torch.reshape(vectors, split_dim_shape + (self.attention_head_size,))
/home/patrick/python_bin/transformers/modeling_reformer.py:120: mem 0.000B:             logits = self._split_dim_by(logits, self.num_hashes, sequence_length).unsqueeze(-1)
/home/patrick/python_bin/transformers/modeling_reformer.py:311: mem 0.000B:         batch_size = vectors.shape[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:312: mem 0.000B:         split_dim_shape = (batch_size, self.num_attention_heads, dim_factor_1, dim_factor_2)
/home/patrick/python_bin/transformers/modeling_reformer.py:314: mem 0.000B:         if len(vectors.shape) == 4:
/home/patrick/python_bin/transformers/modeling_reformer.py:316: mem 0.000B:         elif len(vectors.shape) == 3:
/home/patrick/python_bin/transformers/modeling_reformer.py:317: mem 0.000B:             return torch.reshape(vectors, split_dim_shape)
/home/patrick/python_bin/transformers/modeling_reformer.py:122: mem 20.000MB:             probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
/home/patrick/python_bin/transformers/modeling_reformer.py:123: mem 64.000MB:             out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
/home/patrick/python_bin/transformers/modeling_reformer.py:125: mem 0.000B:         assert out_vectors.shape == (batch_size, self.num_attention_heads, sequence_length, self.attention_head_size)
/home/patrick/python_bin/transformers/modeling_reformer.py:127: mem 0.000B:         out_vectors = self._transpose_for_output(out_vectors)
/home/patrick/python_bin/transformers/modeling_reformer.py:307: mem 0.000B:         x = x.permute(0, 2, 1, 3)
/home/patrick/python_bin/transformers/modeling_reformer.py:308: mem 0.000B:         return torch.reshape(x, (x.size()[0], -1, self.num_attention_heads * self.attention_head_size))
/home/patrick/python_bin/transformers/modeling_reformer.py:128: mem 0.000B:         outputs = (out_vectors, attention_probs) if self.output_attentions else (out_vectors,)
/home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -528.000MB:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:367: mem 0.000B:         attention_output = self.output(self_attention_outputs[0], prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:340: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:341: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:343: mem 64.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:344: mem -64.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:368: mem 0.000B:         outputs = (attention_output,) + self_attention_outputs[1:]  # add attentions if we output them
/home/patrick/python_bin/transformers/modeling_reformer.py:369: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_reformer.py:433: mem 0.000B:         attention_output = attention_outputs[0]
/home/patrick/python_bin/transformers/modeling_reformer.py:434: mem 0.000B:         output = self.feed_forward(attention_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:410: mem 66.000MB:         norm_attention_output = self.layer_norm(attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:411: mem 0.000B:         dense_output = self.dense(norm_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:383: mem 128.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:384: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:385: mem 0.000B:         hidden_states = self.act_fn(hidden_states)
/home/patrick/python_bin/transformers/activations.py:18: mem 384.000MB:     return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
/home/patrick/python_bin/transformers/modeling_reformer.py:386: mem 0.000B:         return hidden_states
/home/patrick/python_bin/transformers/modeling_reformer.py:412: mem 0.000B:         output = self.output(dense_output, prev_attention_output)
/home/patrick/python_bin/transformers/modeling_reformer.py:396: mem 64.000MB:         hidden_states = self.dense(hidden_states)
/home/patrick/python_bin/transformers/modeling_reformer.py:397: mem 0.000B:         hidden_states = nn.functional.dropout(hidden_states, self.dropout, self.training)
/home/patrick/python_bin/transformers/modeling_reformer.py:398: mem 64.000MB:         output = (hidden_states + input_tensor)
/home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -64.000MB:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:413: mem 0.000B:         return output
/home/patrick/python_bin/transformers/modeling_reformer.py:436: mem 0.000B:         outputs = (attention_output, output) + attention_outputs[1:]
/home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -5.277GB:         return outputs

Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:222: mem 2.000GB:             mask = torch.lt(query_info.unsqueeze(-1), key_value_info.unsqueeze(-2)).long().to(query_info.device)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:213: mem 1.500GB:         query_key_dots = torch.matmul(query_vectors, key_vectors.transpose(-1, -2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:244: mem 1.000GB:         dots = torch.exp(query_key_dots - logits)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:283: mem 528.000MB:                 slices.append(torch.cat([vectors[:, :, i:, ...], vectors[:, :, :i, ...]], dim=2))
=> /home/patrick/python_bin/transformers/modeling_reformer.py:284: mem 528.000MB:         return torch.cat(slices, dim=3)
=> /home/patrick/python_bin/transformers/modeling_reformer.py:298: mem 512.000MB:         vectors = vectors.repeat(1, 1, self.num_hashes, 1)

Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_reformer.py:344: mem -64.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:399: mem -64.000MB:         return output
=> /home/patrick/python_bin/transformers/modeling_reformer.py:190: mem -272.000MB:         return offset_buckets
=> /home/patrick/python_bin/transformers/modeling_reformer.py:129: mem -528.000MB:         return outputs
=> /home/patrick/python_bin/transformers/modeling_reformer.py:264: mem -2.531GB:         return out_vectors, logits, dots
=> /home/patrick/python_bin/transformers/modeling_reformer.py:437: mem -5.277GB:         return outputs

Total memory increase: 8.766GB
Going through model with sequence of shape torch.Size([8, 32768, 64])
=========== RESULTS ===========
	======= MODEL CHECKPOINT: ReformerLayer =======
		===== BATCH SIZE: 1 =====
		ReformerLayer/1/1024: 0.002s 48.891MB
		ReformerLayer/1/2048: 0.002s 124.305MB
		ReformerLayer/1/4096: 0.003s 154.000MB
		ReformerLayer/1/8192: 0.004s 260.000MB
		ReformerLayer/1/16384: 0.008s 560.000MB
		ReformerLayer/1/32768: 0.015s 1.084GB
		===== BATCH SIZE: 8 =====
		ReformerLayer/8/1024: 0.004s 260.000MB
		ReformerLayer/8/2048: 0.007s 568.000MB
		ReformerLayer/8/4096: 0.014s 1.082GB
		ReformerLayer/8/8192: 0.029s 2.149GB
		ReformerLayer/8/16384: 0.055s 4.357GB
		ReformerLayer/8/32768: 0.11s 8.766GB
